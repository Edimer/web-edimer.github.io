{
  "hash": "46f207f799709aae4d1fd18a910925b6",
  "result": {
    "markdown": "---\ntitle: \"XGBoost con R\"\nauthor: \"Edimer (Sidereus)\"\ndate: \"05-01-2021\"\ndescription: \"Algortimo XGBoost con R para resolver problemas de aprendizaje supervisado (clasificación).\"\ncategories:\n  - R\n  - xgboost\n  - Gradient Boosting\n  - ML\nimage: \"img2.png \"\nlang: es\ncss: estilo.css\nformat: \n  html:\n    toc: true\n    toc-title: \"Tabla de contenido\"\n    smooth-scroll: true\n    code-fold: true\n    df-print: paged\n    toc-location: left\n    number-depth: 4\n    code-copy: true\n    highlight-style: github\n    code-tools: \n      source: true \n    code-link: true \n---\n\n\n\n\n## Documentación `xgboost`\n\n- [Documentación oficial `xgboost`](https://xgboost.readthedocs.io/en/latest/#)\n- [Biblioteca `xgboost` en R](https://cran.r-project.org/web/packages/xgboost/xgboost.pdf)\n- [Tutorial de `xgboost` en R](https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html)\n\n# Datos de ejemplo\n\nEn este documento se muestra cómo implementar el algoritmo *XGBoost* con R a través de la biblioteca que lleva el mismo nombre y haciendo uso del [`tidymodels`.](https://www.tidymodels.org/) Para ejemplificar el ajuste de *modelos de clasificación* se obtuvieron datos de ejemplo aplicado en ciencias animales, específicamente en la detección de patrones de infección de mastitis en vacas.\n\n## Detección de mastitis\n\n- [Artículo publicado en PLOS ONE](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0249136)\n\n<center>\n<img src = \"img/paper_classification.PNG\" width = 750/>\n</center>\n\n\n# Requisitos previos\n\n- Para replicar este documento es necesario instalar las siguientes bibliotecas:\n  - **`tidyverse`:** manipulación  y visualización de datos.\n  - **`readxl`:** lectura de datos en formato de Excel.\n  - **`janitor`:** manipulación de datos.\n  - **`visdat`:** análisis exploratorio de datos. \n  - **`tidymodels`:** entrenamiento y evaluación de modelos de machine learning.\n  - **`xgboost`:** algoritmo `xgboost`. \n  - **`doParallel`:** procesamiento en paralelo (cómputo distribuido).\n  - **`parallel`** procesamiento en paralelo (cómputo distribuido).\n  - **`vip`:** calcular importancia de variables.\n  - **`ggsci`:** paletas de colores.\n  - **`ggforce`** complemento para gráficos.\n  - **`FactoMineR`:** análisis multivariado.\n  - **`knitr`:** presentación de tablas (`data.frame`).\n- Descargar los datos para ejemplo:\n  - [Datos para detección de mastitis](https://www.nature.com/articles/s41598-020-61126-8#Sec15)\n- **Nota:** es importante mencionar que cuando cargamos la *meta-biblioteca* `tidymodels` se activan otras bibliotecas que usaremos más adelante.\n\n# Bibliotecas\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(visdat)\nlibrary(janitor)\nlibrary(FactoMineR)\nlibrary(tidymodels)\nlibrary(xgboost)\nlibrary(doParallel)\nlibrary(parallel)\nlibrary(vip)\nlibrary(ggsci)\nlibrary(ggforce)\nlibrary(knitr)\n```\n:::\n\n\n# Tema para `ggplot2`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmi_temagg <- theme_minimal() +\n  theme(axis.text.x = element_text(color = \"black\"),\n        axis.text.y = element_text(color = \"black\"),\n        strip.background = element_rect(fill = \"gray5\"),\n        strip.text = element_text(color = \"white\", size = 12),\n        legend.position = \"top\")\n\ntheme_set(mi_temagg)\n```\n:::\n\n\n# Detección de mastitis\n\n- La variable respuesta está identificada como `diagnosis`. En principio tiene 4 niveles, sin embargo, para el ejemplo fueron filtrados sólo los niveles `EDP` (transmisión en el período seco - sin lactancia) y `EL` (transmisión en período de lactancia).\n- La base de datos consta de 1000 observaciones y 229 variables, es decir, que existen 228 variables predictoras. Cuando se aplicó el filtro quedaron 914 observaciones.\n\n## Base de datos\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmastitis <- read_csv(\"data/mastitis.csv\") %>% \n  clean_names() %>% \n  filter(diagnosis %in% c(\"EDP\", \"EL\"))\n\nmastitis %>% \n  select(diagnosis, 1:5) %>%\n  head() %>%\n  kable(caption = \"6 primeras filas de la base de datos con 6 columnas\")\n```\n\n::: {.cell-output-display}\nTable: 6 primeras filas de la base de datos con 6 columnas\n\n|diagnosis |quarter_dates_q0 | no_recordings_q0| l_1_q0_bmscc_000_cells_ml| l_1_q0_percent_chronic| l_1_q0_percent_200k|\n|:---------|:----------------|----------------:|-------------------------:|----------------------:|-------------------:|\n|EDP       |06/10/2009       |                0|                         0|                    0.0|                 0.0|\n|EDP       |06/12/2009       |                3|                       284|                   27.8|                40.6|\n|EL        |20/04/2012       |                0|                         0|                    0.0|                 0.0|\n|EL        |16/07/2009       |                3|                       222|                   21.1|                26.3|\n|EL        |16/10/2013       |                3|                       251|                   27.5|                34.3|\n|EDP       |12/01/2013       |                3|                       154|                   12.3|                17.0|\n:::\n:::\n\n\n## Análisis exploratorio\n\n- Frecuencia absoluta para niveles de la variable respuesta.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmastitis %>% \n  count(diagnosis) %>% \n  ggplot(aes(x = diagnosis, y = n)) +\n  geom_col(color = \"dodgerblue3\", fill = \"dodgerblue3\", alpha = 0.8) +\n  geom_label(aes(label = n)) +\n  labs(x = \"Diagnóstico\", y = \"Frecuencia\")\n```\n\n::: {.cell-output-display}\n![Frecuencia absoluta de variable respuesta](XGBoost_R_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n- Distribuciones de algunas variables numéricas: como el número de variables es alto, selecciono al azar 8 de ellas para construir el gráfico.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Números al azar\nset.seed(1992)\nvariables_azar <- sample(x = 228, size = 8, replace = FALSE)\n\nmastitis %>% \n  select(diagnosis, variables_azar) %>% \n  pivot_longer(cols = -diagnosis) %>% \n  ggplot(aes(x = value, fill = diagnosis, color = diagnosis)) +\n  facet_wrap(~name, scales = \"free\", ncol = 4) +\n  geom_density(alpha = 0.8) +\n  scale_x_log10() +\n  scale_color_jama() +\n  scale_fill_jama() +\n  labs(x = \"\", y = \"Densidad\", color = \"Diagnóstico\",\n       fill = \"Diagnóstico\")\n```\n\n::: {.cell-output-display}\n![Densidades de 8 variables al azar (transformación con logaritmo en base 10)](XGBoost_R_files/figure-html/unnamed-chunk-5-1.png){width=768}\n:::\n:::\n\n\n- Verificamos si existen valores ausentes. Se observan algunas filas con valores `NA`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvis_miss(mastitis)\n```\n\n::: {.cell-output-display}\n![Valores ausentes en el conjunto de datos](XGBoost_R_files/figure-html/unnamed-chunk-6-1.png){width=768}\n:::\n:::\n\n\n- Con la finalidad de evidenciar si existe algún patrón de asociación subyacente en el total de variables, se realizó análisis de componentes principales y se grafican los dos primeros componentes. No existe algún comportamiento de agrupación al reducir la dimensión a los dos primeros componentes. Se observó que la retención de variabilidad de estas dos coordenadas apenas alcanzó el 30% aproximadamente.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Análisis de componentes principales\ndatos_pca <- mastitis\npca <- PCA(X = datos_pca %>%\n             select(where(is.numeric)),\n           scale.unit = TRUE,\n           graph = FALSE)\n\n# Agregando componentes a la base de datos\ndatos_pca$cp1 <- pca$ind$coord[, 1]\ndatos_pca$cp2 <- pca$ind$coord[, 2]\n\n# Gráfico de las 2 primeras componentes\ndatos_pca %>% \n  ggplot(aes(x = cp1, y = cp2, color = diagnosis)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"black\", linetype = 2) +\n  geom_vline(xintercept = 0, color = \"black\", linetype = 2) +\n  scale_color_jama() +\n  labs(x = \"CP1 (15.74 %)\", y = \"CP2 (14.09 %)\", color = \"Diagnóstico\")\n```\n\n::: {.cell-output-display}\n![Componente principal 1 vs componente principal 2](XGBoost_R_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n- Los resultados exploratorios sugieren cuatro cosas importantes:\n  - El problema de clasificación bajo análisis podría ser denominado de *clases balanceadas*, ya que las frecuencias absolutas son similares para cada nivel a predecir.\n  - Algunas variables predictoras no tienen comportamiento gaussiano. Parte de la estrategia del análisis, en estos casos, podría ser la implementación de algún tipo de transformación previo al entrenamiento de los modelos, no obstante, este paso no se aplicará dada la robustez que presenta el algoritmo *XGBoost* frente a distribuciones asimétricas.\n  - Existen observaciones con valores ausentes para una o más variables. Algunos algoritmos como la *regresión logística* o los *bosques aleatorios*, no permiten la inclusión de valores `NA` al entrenar los modelos, sin embargo, con *XGBoost* no existe este inconveniente, puesto que soporta valores vacíos.\n  - El análisis de componentes principales no muestra alguna tendencia de agrupación entre las clases evaluadas. La retención de variabilidad de las tres primeras componentes no supera el 50%, resultado que podría sugerir que las relaciones de tipo lineal no son plausibles en este conjunto de datos.\n\n## Train y Test\n\n- En este ejemplo los datos fueron divididos en *train* y *test* con proporciones de 80 y 20 %, respectivamente. Se utiliza muestreo estratificado en función de la variable respuesta. Se eliminaron 4 variables que tienen información de fechas; aunque podrían ser tratadas de alguna manera especial, en este caso no fueron tenidas en cuenta.\n- La biblioteca [`rsample`](https://rsample.tidymodels.org/) permite realizar la división.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Variables fechas\nvariables_fechas <- mastitis %>% \n  select(is.character, -diagnosis) %>% \n  names()\n\n# Datos para modelos\ndata_modelos <- mastitis %>% select(-variables_fechas)\n\nset.seed(1992)\nparticiones <- initial_split(data = data_modelos, prop = 0.80, strata = diagnosis)\ntrain <- training(particiones)\ntest <- testing(particiones)\n```\n:::\n\n\n- Podemos ver el objeto `particiones` que proporciona información de la partición de datos. Los modelos son entrenados con 733 observaciones y con 181 se evalúa el desempeño de los mismos.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparticiones\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Training/Testing/Total>\n<731/183/914>\n```\n:::\n:::\n\n\n## Validación cruzada\n\n- Se utiliza validación cruzada *k-fold* con $k = 10$.\n- La biblioteca [`rsample`](https://rsample.tidymodels.org/) permite configurar diferentes métodos de validación cruzada.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1992)\nconfig_cv <- vfold_cv(data = train, v = 10)\n```\n:::\n\n\n## Preprocesamiento\n\n- En este ejemplo no se hará énfasis en las estrategias de preprocesamiento o ingeniería de características, sin embargo, serán comparados algoritmos con imputación de valores ausentes (con método k vecinos más cercanos) respecto a algoritmos sin imputación. \n- La biblioteca [`recipes`](https://recipes.tidymodels.org/) permite realizar múltiples tareas de preprocesamiento e ingeniería de características.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nno_impute <- recipe(diagnosis ~ ., data = train)\nsi_impute <- recipe(diagnosis ~ ., data = train) %>% \n  step_impute_knn(all_predictors())\n```\n:::\n\n\n## Modelo XGBoost\n\n- El algoritmo *XGboost* tiene múltiples hiperparámetros que pueden ser sintonizados, sin embargo, en este ejemplo sólo ser hará tuning sobre los siguientes:\n  - `mtry`: número de predictores que se muestrearán aleatoriamente en cada división al crear los modelos.\n  - `min_n`: número mínimo de observaciones requeridas en un nodo para que se produzca la división.\n  - `tree_depth`: profundidad máxima del árbol (número de divisiones).\n- El número de árboles (`trees`) se estableció en 1000.\n- La tasa de aprendizaje (`learn_rate`) se estableció en 0.1.\n- La proporción de observaciones muestreadas (`sample_size`) en cada rutina de ajuste se estableció en 0.8.\n- Los demás hiperparámetros se mantienen por defecto.\n- Para más información acerca de los hiperparámetros que permite ajustar [`parsnip`](https://parsnip.tidymodels.org/), consultar [este enlace.](https://parsnip.tidymodels.org/reference/boost_tree.html)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelo_xgboost <- boost_tree(mtry = tune(),\n                             min_n = tune(),\n                             tree_depth = tune(),\n                             trees = 1000,\n                             learn_rate = 0.1,\n                             sample_size = 0.8) %>% \n  set_engine(\"xgboost\") %>% \n  set_mode(\"classification\")\n```\n:::\n\n\n## Flujos de trabajo (*pipelines*)\n\n- Los flujos de trabajo son una manera flexible de trabajar con *tidymodels*. Se fundamenta en la misma idea de los *pipelines* de [scikit-learn](https://scikit-learn.org/stable/) de Python. Es posible construir nuestro flujo de trabajo con recetas y modelos declarados previamente.\n- Consultar más información de la biblioteca [workflows.](https://workflows.tidymodels.org/) \n\n- Flujo de trabajo sin imputación:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_no_impute <- workflow() %>% \n  add_recipe(no_impute) %>% \n  add_model(modelo_xgboost)\n```\n:::\n\n\n- Flujo de trabajo con imputación:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_si_impute <- workflow() %>% \n  add_recipe(si_impute) %>% \n  add_model(modelo_xgboost)\n```\n:::\n\n\n## Grid \n\n- En este caso se utiliza la cuadrícula a través de diseños de llenado de espacio (*space-filling designs*), los cuales intentan encontrar una configuración de puntos (combinaciones) que cubren el espacio de parámetros con menor probabilidad de valores que se traslapan. Para este ejemplo el tamaño de la cuadrícula fue de 10.\n- Para este caso particular se usan diseños de máxima entropía, descritos en el año 1987 por Shewry y Wynn en el artículo *\"Maximum Entropy Sampling\".* También podrían ser implementados diseños de hipercubos latinos o diseños de proyección máxima. Si no se desea utilizar alguno de estos métodos, podría ser implementada una cuadrícula regular a través de métodos aleatorios (*grid random*).\n- Mayor información en la página web de la biblioteca [dials.](https://dials.tidymodels.org/index.html)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Parámetros para tuning\nparams_xgb <- parameters(\n  finalize(mtry(), x = train[, -1]),\n  min_n(range = c(2L, 50L)),\n  tree_depth(range = c(3L, 8L))\n)\n\n# Grid\nset.seed(2021)\ngrid_xgb <- params_xgb %>% \n  grid_max_entropy(size = 10)\n```\n:::\n\n\n- A continuación se muestra la cuadrícula de búsqueda de los mejores hiperparámetros. Se evidencia que los puntos no se solapan, de tal manera que el espacio de búsqueda no es redundante.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_xgb %>% \n  ggplot(aes(x = .panel_x, y = .panel_y)) +\n  facet_matrix(vars(mtry, min_n, tree_depth), layer.diag = 2) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![Cuadrícula de tamaño 10 con método de máxima entropía](XGBoost_R_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n## Tuning con `tidymodels`\n\n- La función `tune_grid()` de la biblioteca [tune](https://tune.tidymodels.org/) permite evaluar los modelos con cada combinación de paramétros establecidos previamente en la cuadrícula.\n- Tuning sin imputación:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregisterDoParallel(parallel::detectCores() - 1) # Inicio Paralelización\n\nset.seed(2021)\ntuned_no_impute <- tune_grid(\n  object = wf_no_impute,\n  resamples = config_cv,\n  grid = grid_xgb\n)\n\nstopImplicitCluster() # Fin Paralelización\n```\n:::\n\n\n- Tuning con imputación:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregisterDoParallel(parallel::detectCores() - 1) # Inicio Paralelización\n\nset.seed(2021)\ntuned_si_impute <- tune_grid(\n  object = wf_si_impute,\n  resamples = config_cv,\n  grid = grid_xgb\n)\n\nstopImplicitCluster() # Fin Paralelización\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n## Resultados *Accuracy*\n\n- Resultados de *Accuracy* en modelos sin imputación. La precisión más alta se consigue con aproximadamente 150 variables (`mtry`), menos de 10 observaciones para que se produzca la división del árbol (`min_n`) y profunidad de más o menos 6 (`tree_depth`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntuned_no_impute %>% \n  collect_metrics() %>% \n  filter(.metric == \"accuracy\") %>% \n  ggplot(aes(x = mtry, y = min_n, size = tree_depth, color = mean)) +\n  geom_point() +\n  scale_color_viridis_c() +\n  labs(color = \"Accuracy\")\n```\n\n::: {.cell-output-display}\n![](XGBoost_R_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n- Resultados de *Accuracy* en modelos con imputación. La precisión más alta se consigue con aproximadamente 25 variables (`mtry`), poco menos de 50 observaciones para que se produzca la división del árbol (`min_n`) y profunidad de más o menos 4 (`tree_depth`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntuned_si_impute %>% \n  collect_metrics() %>% \n  filter(.metric == \"accuracy\") %>% \n  ggplot(aes(x = mtry, y = min_n, size = tree_depth, color = mean)) +\n  geom_point() +\n  scale_color_viridis_c() +\n  labs(color = \"Accuracy\")\n```\n\n::: {.cell-output-display}\n![](XGBoost_R_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n- Se observa que los mejores hiperparámetros para los algoritmos entrenados con y sin imputación, discrepan considerablemente. Aunque en este caso realicé la evaluación del desempeño de los modelos basado en la métrica *Accuracy*, es posible utilizar cualquier otra para problemas de clasificación.\n\n## Mejores hiperparámetros\n\n- Mejores hiperparámetros en modelos sin imputación:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmejor_no_impute <- tuned_no_impute %>% \n  select_best(metric = \"accuracy\")\n\nmejor_no_impute\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"mtry\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"min_n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"tree_depth\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\".config\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"149\",\"2\":\"9\",\"3\":\"7\",\"4\":\"Preprocessor1_Model09\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n- Mejores hiperparámetros en modelos con imputación:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmejor_si_impute <- tuned_si_impute %>% \n  select_best(metric = \"accuracy\")\n\nmejor_si_impute\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"mtry\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"min_n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"tree_depth\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\".config\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"25\",\"2\":\"49\",\"3\":\"5\",\"4\":\"Preprocessor1_Model01\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n## Ajuste final\n\n- Modelo sin imputación:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_no_impute <- finalize_workflow(\n  x = wf_no_impute,\n  parameters = mejor_no_impute\n) %>% \n  fit(data = train)\n```\n:::\n\n\n- Modelo con imputación:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_si_impute <- finalize_workflow(\n  x = wf_si_impute,\n  parameters = mejor_si_impute\n) %>% \n  fit(data = train)\n```\n:::\n\n\n## Predicciones Train\n\n- Modelo sin imputación:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_no_impute_train <- final_no_impute %>% \n  predict(new_data = train, type = \"class\")\n```\n:::\n\n\n- Modelo con imputación:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_si_impute_train <- final_si_impute %>% \n  predict(new_data = train, type = \"class\") \n```\n:::\n\n\n## Predicciones Test\n\n- Modelo sin imputación:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_no_impute_test <- final_no_impute %>% \n  predict(new_data = test, type = \"class\")\n```\n:::\n\n\n- Modelo con imputación:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_si_impute_test <- final_si_impute %>% \n  predict(new_data = test, type = \"class\") \n```\n:::\n\n\n## Matriz de confusión Train\n\n- Matriz de confusión modelo sin imputación:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(\n  predicho = as.factor(pred_no_impute_train$.pred_class),\n  real = as.factor(train$diagnosis)\n) %>% \n  conf_mat(truth = real, estimate = predicho) %>% \n  pluck(1) %>% \n  as_tibble()  %>% \n  ggplot(aes(x = Prediction, y = Truth, alpha = n)) +\n  geom_tile(show.legend = FALSE) +\n  geom_text(aes(label = n), colour = \"white\", alpha = 1, size = 8)\n```\n\n::: {.cell-output-display}\n![Matriz de confusión en train - modelo sin imputación](XGBoost_R_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\n- Matriz de confusión modelo con imputación:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(\n  predicho = as.factor(pred_si_impute_train$.pred_class),\n  real = as.factor(train$diagnosis)\n) %>% \n  conf_mat(truth = real, estimate = predicho) %>% \n  pluck(1) %>% \n  as_tibble()  %>% \n  ggplot(aes(x = Prediction, y = Truth, alpha = n)) +\n  geom_tile(show.legend = FALSE) +\n  geom_text(aes(label = n), colour = \"white\", alpha = 1, size = 8)\n```\n\n::: {.cell-output-display}\n![Matriz de confusión en train - modelo con imputación](XGBoost_R_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\n## Matriz de confusión Test\n\n- Matriz de confusión modelo sin imputación:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(\n  predicho = as.factor(pred_no_impute_test$.pred_class),\n  real = as.factor(test$diagnosis)\n) %>% \n  conf_mat(truth = real, estimate = predicho) %>% \n  pluck(1) %>% \n  as_tibble()  %>% \n  ggplot(aes(x = Prediction, y = Truth, alpha = n)) +\n  geom_tile(show.legend = FALSE) +\n  geom_text(aes(label = n), colour = \"white\", alpha = 1, size = 8)\n```\n\n::: {.cell-output-display}\n![Matriz de confusión en test - modelo sin imputación](XGBoost_R_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n- Matriz de confusión modelo con imputación:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(\n  predicho = as.factor(pred_si_impute_test$.pred_class),\n  real = as.factor(test$diagnosis)\n) %>% \n  conf_mat(truth = real, estimate = predicho) %>% \n  pluck(1) %>% \n  as_tibble()  %>% \n  ggplot(aes(x = Prediction, y = Truth, alpha = n)) +\n  geom_tile(show.legend = FALSE) +\n  geom_text(aes(label = n), colour = \"white\", alpha = 1, size = 8)\n```\n\n::: {.cell-output-display}\n![Matriz de confusión en test - modelo con imputación](XGBoost_R_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n## Desempeño de modelos\n\n- Genero una base de datos con los resultados de las clases predichas en cada tipo de modelo (con y sin imputación) para los conjuntos de entrenamiento y prueba.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntabla_accuracy <- data.frame(\n  predicho = pred_no_impute_train$.pred_class,\n  real = train$diagnosis,\n  datos = \"Train\",\n  tipo = \"Sin imputación\"\n) %>%\n  bind_rows(\n    data.frame(\n      predicho = pred_si_impute_train$.pred_class,\n      real = train$diagnosis,\n      datos = \"Train\",\n      tipo = \"Con imputación\"\n    )\n  ) %>%\n  bind_rows(\n    data.frame(\n      predicho = pred_no_impute_test$.pred_class,\n      real = test$diagnosis,\n      datos = \"Test\",\n      tipo = \"Sin imputación\"\n    )\n  ) %>% \n  bind_rows(\n    data.frame(\n      predicho = pred_si_impute_test$.pred_class,\n      real = test$diagnosis,\n      datos = \"Test\",\n      tipo = \"Con imputación\"\n    )\n  ) %>% \n  mutate(across(where(is.character), as.factor))\n\ntabla_accuracy %>%\n  group_by(datos, tipo) %>% \n  summarise(accuracy = accuracy_vec(truth = real, estimate = predicho)) %>% \n  kable(caption = \"Accuracy en train y test para dos modelos XGBoost\")\n```\n\n::: {.cell-output-display}\nTable: Accuracy en train y test para dos modelos XGBoost\n\n|datos |tipo           |  accuracy|\n|:-----|:--------------|---------:|\n|Test  |Con imputación | 0.7540984|\n|Test  |Sin imputación | 0.7814208|\n|Train |Con imputación | 0.8481532|\n|Train |Sin imputación | 0.9972640|\n:::\n:::\n\n- Gráfico *Accuracy:* la capacidad predictiva es superior en el modelo que fue entrenado sin acudir a la imputación de datos.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntabla_accuracy %>%\n  group_by(datos, tipo) %>% \n  summarise(accuracy = accuracy_vec(truth = real, estimate = predicho)) %>% \n  ggplot(aes(x = tipo, y = accuracy, color = datos, fill = datos)) +\n  geom_col(position = \"dodge\", alpha = 0.8) +\n  scale_color_jama() +\n  scale_fill_jama() +\n  labs(x = \"Preprocesamiento\", y = \"Accuracy\",\n       color = \"\", fill = \"\")\n```\n\n::: {.cell-output-display}\n![Accuracy en train y test para dos modelos XGBoost](XGBoost_R_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n\n\n## Importancia de variables\n\n- 10 variables de mayor importancia en modelo sin imputación:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_no_impute %>% \n  pull_workflow_fit() %>%\n  vip(geom = \"point\", n = 10)\n```\n\n::: {.cell-output-display}\n![Importancia de variables - modelo sin imputación](XGBoost_R_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n\n- 10 variables de mayor importancia en modelo sin imputación:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_si_impute %>% \n  pull_workflow_fit() %>%\n  vip(geom = \"point\", n = 10)\n```\n\n::: {.cell-output-display}\n![Importancia de variables - modelo con imputación](XGBoost_R_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\n# Referencias\n\n- Hyde, R.M., Down, P.M., Bradley, A.J. et al. *\"Automated prediction of mastitis infection patterns in dairy herds using machine learning\"*. Sci Rep 10, 4289 (2020). https://doi.org/10.1038/s41598-020-61126-8\n- Chen Tianqi, Guestrin Carlos. *\"XGBoost: A Scalable Tree Boosting System\"*. CoRR, Vol 1603.02754 (2016). https://arxiv.org/abs/1603.02754\n- Boehmke Bradley, Greenwell Brandon. *\"Hands-On Machine Learning with R\"*. Chapman and Hall/CRC (2019). https://bradleyboehmke.github.io/HOML/gbm.html\n- Shewry, M, and H Wynn. *\"Maximum Entropy Sampling.\"* Journal of Applied Statistics 14 (2): 165–70  (1987). https://doi.org/10.1080/02664768700000020\n- Kuhn Max, Silge Julia. *\"Tidy Modeling with R\"*. (2020). https://www.tmwr.org/",
    "supporting": [
      "XGBoost_R_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}