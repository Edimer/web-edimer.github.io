[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Informaci√≥n",
    "section": "",
    "text": "¬°Soy m√°s amigo de Salviati que de Simplicio, aunque respeto y admiro la visi√≥n neutral de Sagredo!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning con R y Python",
    "section": "",
    "text": "R\n\n\nSimulaci√≥n\n\n\nMatem√°ticas\n\n\n\n\nEn este documento se ejemplifican simulaciones con R para la ecuaci√≥n diferencial log√≠stica. üòÅüêüüêíüê§üêñüêì‚òòÔ∏èüêÑüòÅ\n\n\n\n\n\n\n15 nov 2022\n\n\nEdimer (Sidereus)\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nxgboost\n\n\nGradient Boosting\n\n\nML\n\n\n\n\nAlgortimo XGBoost con R para resolver problemas de aprendizaje supervisado (clasificaci√≥n).\n\n\n\n\n\n\n1 may 2021\n\n\nEdimer (Sidereus)\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nTree Decision\n\n\nML\n\n\n\n\nEjemplo de √°rboles de decisi√≥n en machine learning supervisado para clasificaci√≥n. Uso de las bibliotecas rpart, rpart.plot y caret en perfilamiento de riesgo crediticio.\n\n\n\n\n\n\n18 jul 2020\n\n\nEdimer (Sidereus)\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\ncaret\n\n\nML\n\n\n\n\nAlgoritmos de machine learning con caret y R. Entrenamiento de modelos random forest y support vector machine en problemas de clasificaci√≥n supervisada.\n\n\n\n\n\n\n23 mar 2020\n\n\nEdimer (Sidereus)\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\nProgramaci√≥n\n\n\n\n\nUtilizando python desde R con la biblioteca reticulate. Elementos b√°sicos de python para operaciones num√©ricas comunes, visualizaci√≥n y ejemplo corto con scikit-learn.\n\n\n\n\n\n\n20 ene 2020\n\n\nEdimer (Sidereus)\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "posts/caret_r/caret_R.html",
    "href": "posts/caret_r/caret_R.html",
    "title": "Algoritmos de ML con caret en R (1)",
    "section": "",
    "text": "Fuente: predicci√≥n de estrellas p√∫lsar.\n¬øQu√© es un p√∫lsar?"
  },
  {
    "objectID": "posts/caret_r/caret_R.html#variable-respuesta",
    "href": "posts/caret_r/caret_R.html#variable-respuesta",
    "title": "Algoritmos de ML con caret en R (1)",
    "section": "Variable respuesta",
    "text": "Variable respuesta\n\nC√≥digodf_pulsar %>% group_by(pulsar) %>% count() %>% \n  ggplot(data = ., aes(x = pulsar, y = n)) +\n  geom_col(color = \"black\", fill = \"#033660\") +\n  geom_label(aes(label = n)) +\n  labs(x = \"¬øP√∫lsar?\"?\", titl= \"Distribuci√≥n de variable respuesta\"ta\",\n       subtitle = \"0=No\\n1=S√≠\"))++\n  mi_temagg"
  },
  {
    "objectID": "posts/caret_r/caret_R.html#distribuciones",
    "href": "posts/caret_r/caret_R.html#distribuciones",
    "title": "Algoritmos de ML con caret en R (1)",
    "section": "Distribuciones",
    "text": "Distribuciones\n\nC√≥digodf_pulsar %>% \n  gather(key = \"variable\", value = \"valor\", -pulsar) %>% \n  ggplot(data = ., aes(x = valor, fill = pulsar)) +\n  facet_wrap(~variable, scales = \"free\", ncol = 4) +\n  geom_density(alpha = 0.9) +\n  scale_x_log10() +\n  labs(x = \"\", y = \"Densidad\", title = \"Escala logar√≠tmica\"\",\n       fill = \"¬øP√∫lsar?\")\"+ +\n  scale_fill_manual(values = c(\"#790222\", \"#033660\")) +\n  mi_temagg"
  },
  {
    "objectID": "posts/caret_r/caret_R.html#correlaciones",
    "href": "posts/caret_r/caret_R.html#correlaciones",
    "title": "Algoritmos de ML con caret en R (1)",
    "section": "Correlaciones",
    "text": "Correlaciones\n\nC√≥digo# Cargando biblioteca corrplot\nlibrary(corrplot)\n\ndf_pulsar %>% mutate_if(is.numeric, scale)  %>% select_if(is.numeric) %>%\n  cor(method = \"spearman\") %>% \n  corrplot(method = \"pie\", type = \"upper\", order = \"hclust\", diag = FALSE,\n           tl.srt = 35, tl.col = \"black\", tl.cex = 1)"
  },
  {
    "objectID": "posts/caret_r/caret_R.html#random-forest",
    "href": "posts/caret_r/caret_R.html#random-forest",
    "title": "Algoritmos de ML con caret en R (1)",
    "section": "Random Forest",
    "text": "Random Forest\nAlgoritmo\n\nC√≥digo# Algoritmo de random forest\nmodelo_rf <- train(pulsar ~ ., data = df_train, method = \"ranger\")\n\n# Guardando modelo\nsaveRDS(object = modelo_rf, file = \"models_fit/RandomForest.rds\")\n\n\n\nResultados:\n\n\nC√≥digo# Cargando modelo\nmod_rf <- readRDS(\"models_fit/RandomForest.rds\")\n\n# Resultados del modelo\nmod_rf\n\nRandom Forest \n\n12530 samples\n    8 predictor\n    2 classes: '0', '1' \n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 12530, 12530, 12530, 12530, 12530, 12530, ... \nResampling results across tuning parameters:\n\n  mtry  splitrule   Accuracy   Kappa    \n  2     gini        0.9801226  0.8758954\n  2     extratrees  0.9794474  0.8705685\n  5     gini        0.9801169  0.8762727\n  5     extratrees  0.9803843  0.8777407\n  8     gini        0.9795793  0.8731006\n  8     extratrees  0.9804887  0.8788098\n\nTuning parameter 'min.node.size' was held constant at a value of 1\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were mtry = 8, splitrule = extratrees\n and min.node.size = 1.\n\n\nDesempe√±o\n\nMatriz de confusi√≥n en test:\n\n\nC√≥digo# Predicciones en nuevos datos\npredict_rf <- predict(object = mod_rf, newdata = df_test)\n\n# Matriz de confuci√≥nn\nconfusionMatrix(predict_rf, df_test$pulsar, positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 4840   84\n         1   37  407\n                                          \n               Accuracy : 0.9775          \n                 95% CI : (0.9731, 0.9813)\n    No Information Rate : 0.9085          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.8583          \n                                          \n Mcnemar's Test P-Value : 2.892e-05       \n                                          \n            Sensitivity : 0.82892         \n            Specificity : 0.99241         \n         Pos Pred Value : 0.91667         \n         Neg Pred Value : 0.98294         \n             Prevalence : 0.09147         \n         Detection Rate : 0.07582         \n   Detection Prevalence : 0.08271         \n      Balanced Accuracy : 0.91067         \n                                          \n       'Positive' Class : 1"
  },
  {
    "objectID": "posts/caret_r/caret_R.html#svm",
    "href": "posts/caret_r/caret_R.html#svm",
    "title": "Algoritmos de ML con caret en R (1)",
    "section": "SVM",
    "text": "SVM\n\nSe utiliza el m√©todo svmRadial que est√° contenido en la biblioteca kernlab.\nLa configuraci√≥n est√° por defecto.\nEste algoritmo permite ajustar hiperpar√°metros sigma y C (costo).\nDocumentaci√≥n kernlab.\n\nAlgoritmo\n\nC√≥digo# Algoritmo\nmodelo_svmR <- train(pulsar ~ ., data = df_train, method = \"svmRadial\")\n\n# Guardando modelo\nsaveRDS(object = modelo_svmR, file = \"models_fit/SVM_Radial.rds\")\n\n\n\nResultados:\n\n\nC√≥digo# Cargando modelo\nmod_svmR <- readRDS(\"models_fit/SVM_Radial.rds\")\n\n# Resultados del modelo\nmod_svmR\n\nSupport Vector Machines with Radial Basis Function Kernel \n\n12530 samples\n    8 predictor\n    2 classes: '0', '1' \n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 12530, 12530, 12530, 12530, 12530, 12530, ... \nResampling results across tuning parameters:\n\n  C     Accuracy   Kappa    \n  0.25  0.9785764  0.8629191\n  0.50  0.9785329  0.8635606\n  1.00  0.9787235  0.8654589\n\nTuning parameter 'sigma' was held constant at a value of 0.4893064\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were sigma = 0.4893064 and C = 1.\n\n\nDesempe√±o\n\nC√≥digopredict_svmR <- predict(object = mod_svmR, newdata = df_test)\nconfusionMatrix(predict_svmR, df_test$pulsar, positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 4852   95\n         1   25  396\n                                          \n               Accuracy : 0.9776          \n                 95% CI : (0.9733, 0.9814)\n    No Information Rate : 0.9085          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.8563          \n                                          \n Mcnemar's Test P-Value : 2.999e-10       \n                                          \n            Sensitivity : 0.80652         \n            Specificity : 0.99487         \n         Pos Pred Value : 0.94062         \n         Neg Pred Value : 0.98080         \n             Prevalence : 0.09147         \n         Detection Rate : 0.07377         \n   Detection Prevalence : 0.07843         \n      Balanced Accuracy : 0.90070         \n                                          \n       'Positive' Class : 1"
  },
  {
    "objectID": "posts/caret_r/caret_R.html#comparaci√≥n-de-modelos",
    "href": "posts/caret_r/caret_R.html#comparaci√≥n-de-modelos",
    "title": "Algoritmos de ML con caret en R (1)",
    "section": "Comparaci√≥n de modelos",
    "text": "Comparaci√≥n de modelos\n\nC√≥digomod_svmR$resample %>% \n  select(-Resample) %>% \n  mutate(Modelo = \"SVM\") %>% \n  bind_rows(mod_rf$resample) %>% \n  select(-Resample) %>% \n  replace_na(list(Modelo = \"Random Forest\")) %>% \n  gather(key = \"Medida\", value = \"Valor\", -Modelo) %>% \n  ggplot(data = ., aes(x = Modelo, y = Valor, fill = Modelo)) +\n  facet_wrap(~Medida, scales = \"free\", ncol = 2) +\n  geom_violin(alpha = 0.9) +\n  stat_summary(fun = mean, geom = \"point\", pch = 19) +\n  labs(y = \"\", title = \"Comparaci√≥n de modelos\"\",\n       subtitle = \"Predicci√≥n de estrellas p√∫lsar\")\"+ +\n  scale_fill_manual(values =  c(\"#790222\", \"#033660\")) +\n  mi_temagg +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/chaos_theory_r/growth-simulation.html#parametrizaci√≥n-en-r",
    "href": "posts/chaos_theory_r/growth-simulation.html#parametrizaci√≥n-en-r",
    "title": "Simulaci√≥n matem√°tica con R: Ecuaci√≥n Log√≠stica",
    "section": "Parametrizaci√≥n en R",
    "text": "Parametrizaci√≥n en R\nFunci√≥n 1\n\nSe construyen dos funciones para realizar las simulaciones: growth() e iterGrowth()\n\nSe definen los siguientes par√°metros para la funci√≥n growth():\n\n\nrate (\\(\\alpha\\)): tasa de crecimento o raz√≥n de cambio. Relaci√≥n de la natalidad con la mortalidad.\n\nxt: tama√±o de la poblaci√≥n inicial (%). \\(0 \\leq x_t \\leq 1\\)\n\n\nRetorno: esta funci√≥n devuelve el c√°lculo de la poblaci√≥n mediante la ecuaci√≥n anterior.\n\n\nSe definen los siguientes par√°metros para la funci√≥n iterGrowth():\n\n\nrate (\\(\\alpha\\)): tasa de crecimento o raz√≥n de cambio. Relaci√≥n de la natalidad con la mortalidad\n\nxt: tama√±o de la poblaci√≥n inicial (%). \\(0 \\leq x_t \\leq 1\\)\n\n\nt: n√∫mero de generaciones (tiempo generacional) para evaluar el crecimiento poblacional.\n\nxinit: es la misma xt, es decir, la poblaci√≥n inicial. Aunque se repite es √∫til para registrar el tama√±o inicial de la poblaci√≥n, ya que xt cambia al realizar la iteraci√≥n recursiva con while().\n\nRetorno: esta funci√≥n devuelve el c√°lculo de la poblaci√≥n hasta el tiempo t, haciendo uso de la funci√≥n growth() y sus par√°metros espec√≠ficos.\n\n\n\n\nC√≥digo# =========== Funci√≥n para simulaci√≥n con ecuaci√≥n log√≠sticatica\n\n# Crecimiento = (rate * xt) * (1 - xt)\ngrowth <- function(rate, xt) {\n  pob = (rate * xt) * (1 - xt)\n  return(pob)\n}\n\n# Funci√≥n recursiva para obtener la poblaci√≥n para cada tiempo \"t\"t\"\niterGrowth <- function(xt, rate, t, xinit) {\n  res = NULL\n  if (t == 0) {\n    res = xt\n    return(res)\n  } else if (t == 1) {\n    res = growth(rate = rate, xt = xt)\n    return(res)\n  } else{\n    i = 1\n    while (i <= (t)) {\n      res[i + 1] = growth(rate = rate, xt = xt)\n      xt = growth(rate = rate, xt = xt)\n      i = i + 1\n    }\n    res[1] = xinit\n    return(res)\n  }\n}\n\n\nFunci√≥n 2\n\nEsta funci√≥n facilita el proceso de conformar un tibble con los resultados de la simulaci√≥n.\n\n\nC√≥digodfGrowth <- function(xt, rate, t, xinit) {\n  res = iterGrowth(xt,\n                   rate,\n                   t,\n                   xinit) %>%\n    enframe() %>%\n    mutate(t = name - 1) %>%\n    select(t, pob = value) %>%\n    mutate(rate = as.factor(rate))\n  return(res)\n}"
  },
  {
    "objectID": "posts/chaos_theory_r/growth-simulation.html#simulaci√≥n-1",
    "href": "posts/chaos_theory_r/growth-simulation.html#simulaci√≥n-1",
    "title": "Simulaci√≥n matem√°tica con R: Ecuaci√≥n Log√≠stica",
    "section": "Simulaci√≥n 1",
    "text": "Simulaci√≥n 1\n\n¬øQu√© pasa en 100 generaciones con una poblaci√≥n cuya natalidad es 1.72 veces mayor que la mortalidad y su poblaci√≥n actual es 62% del m√°ximo te√≥rico? ¬øEn qu√© valor se estabiliza el crecimiento de la poblaci√≥n? ¬øSe extingue la poblaci√≥n?\n\n\nrate: 1.72\n\nxt: 0.62\n\nt: 100\n\nxinit: 0.62\n\n\n\n\nC√≥digor <- 1.72\nxt <- 0.62\nt <- 100\nxinit <- xt\n\niterGrowth(rate = r, xt = xt, t = t, xinit = xinit) %>% \n  enframe() %>% \n  mutate(t = name - 1) %>% \n  select(t, pob = value) %>% \n  ggplot(aes(x = t, y = pob)) +\n  geom_line(color = \"firebrick2\") +\n  labs(x = \"Tasa\",\n       y = \"Estabilidad poblacional\",\n       title = \"Equilibrio poblacional\",\n       subtitle = TeX(r'($x_{t+1}= (\\alpha x_t) \\times (1-x_t)$)'))"
  },
  {
    "objectID": "posts/chaos_theory_r/growth-simulation.html#simulaci√≥n-2",
    "href": "posts/chaos_theory_r/growth-simulation.html#simulaci√≥n-2",
    "title": "Simulaci√≥n matem√°tica con R: Ecuaci√≥n Log√≠stica",
    "section": "Simulaci√≥n 2",
    "text": "Simulaci√≥n 2\n\n¬øQu√© pasa en 100 generaciones con una poblaci√≥n cuya natalidad es 0.72 veces menor que la mortalidad y su poblaci√≥n actual es 62% del m√°ximo te√≥rico? ¬øEn qu√© valor se estabiliza el crecimiento de la poblaci√≥n? ¬øSe extingue la poblaci√≥n?\n\n\nrate: 0.72\n\nxt: 0.62\n\nt: 100\n\nxinit: 0.62\n\n\n\n\nC√≥digor <- 0.72\nxt <- 0.62\nt <- 100\nxinit <- xt\n\niterGrowth(rate = r, xt = xt, t = t, xinit = xinit) %>% \n  enframe() %>% \n  mutate(t = name - 1) %>% \n  select(t, pob = value) %>% \n  ggplot(aes(x = t, y = pob)) +\n  geom_line(color = \"firebrick2\") +\n  labs(x = \"Tasa\",\n       y = \"Equilibrio poblacional\",\n       title = \"Estabilidad poblacional\",\n       subtitle = TeX(r'($x_{t+1}= (\\alpha x_t) \\times (1-x_t)$)'))"
  },
  {
    "objectID": "posts/chaos_theory_r/growth-simulation.html#simulaci√≥n-3",
    "href": "posts/chaos_theory_r/growth-simulation.html#simulaci√≥n-3",
    "title": "Simulaci√≥n matem√°tica con R: Ecuaci√≥n Log√≠stica",
    "section": "Simulaci√≥n 3",
    "text": "Simulaci√≥n 3\n\n¬øQu√© pasa en 100 generaciones con una poblaci√≥n cuya natalidad es igual que la mortalidad y su poblaci√≥n actual es 62% del m√°ximo te√≥rico? ¬øEn qu√© valor se estabiliza el crecimiento de la poblaci√≥n? ¬øSe extingue la poblaci√≥n?\n\n\nrate: 0.72\n\nxt: 0.62\n\nt: 100\n\nxinit: 0.62\n\n\n\n\nC√≥digor <- 1\nxt <- 0.62\nt <- 100\nxinit <- xt\n\niterGrowth(rate = r, xt = xt, t = t, xinit = xinit) %>% \n  enframe() %>% \n  mutate(t = name - 1) %>% \n  select(t, pob = value) %>% \n  ggplot(aes(x = t, y = pob)) +\n  geom_line(color = \"firebrick2\") +\n  labs(x = \"Tasa\",\n       y = \"Equilibrio poblacional\",\n       title = \"Estabilidad poblacional\",\n       subtitle = TeX(r'($x_{t+1}= (\\alpha x_t) \\times (1-x_t)$)'))"
  },
  {
    "objectID": "posts/chaos_theory_r/growth-simulation.html#simulaciones-en-1",
    "href": "posts/chaos_theory_r/growth-simulation.html#simulaciones-en-1",
    "title": "Simulaci√≥n matem√°tica con R: Ecuaci√≥n Log√≠stica",
    "section": "3 simulaciones en 1",
    "text": "3 simulaciones en 1\n\nPodemos graficar las tres simulaciones anteriores en un s√≥lo gr√°fico.\n\n\nC√≥digo# ============== Simulaci√≥n 3 en 11\n\n# Par√°metros espec√≠ficoco\nr_sim1 <- 1.72\nr_sim2 <- 0.72\nr_sim3 <- 1\n\n# Par√°metros generaless\nxt <- 0.62\nt <- 100\nxinit <- xt\n\n# Uniendo datos de simulaciones\nsim_1 <-\n  iterGrowth(\n    rate = r_sim1,\n    xt = xt,\n    t = t,\n    xinit = xinit\n  ) %>%\n  enframe() %>%\n  mutate(t = name - 1) %>%\n  select(t, pob = value) %>% \n  mutate(rate = r_sim1)\n\nsim_2 <-\n  iterGrowth(\n    rate = r_sim2,\n    xt = xt,\n    t = t,\n    xinit = xinit\n  ) %>%\n  enframe() %>%\n  mutate(t = name - 1) %>%\n  select(t, pob = value) %>% \n  mutate(rate = r_sim2)\n\nsim_3 <-\n  iterGrowth(\n    rate = r_sim3,\n    xt = xt,\n    t = t,\n    xinit = xinit\n  ) %>%\n  enframe() %>%\n  mutate(t = name - 1) %>%\n  select(t, pob = value) %>% \n  mutate(rate = r_sim3)\n\nsim_total <-\n  bind_rows(sim_1, sim_2, sim_3)\n\nsim_total %>% \n  mutate(rate = as.factor(rate)) %>% \n  ggplot(aes(x = t, y = pob, color = rate)) +\n  geom_line() +\n  scale_color_manual(values = c(\"#76b49f\", \"#ef3840\", \"#edea6f\")) +\n  labs(x = \"Tasa\",\n       y = \"Equilibrio poblacional\",\n       title = \"Estabilidad poblacional\",\n       subtitle = TeX(r'($x_{t+1}= (\\alpha x_t) \\times (1-x_t)$)'))"
  },
  {
    "objectID": "posts/chaos_theory_r/growth-simulation.html#simulaci√≥n-4",
    "href": "posts/chaos_theory_r/growth-simulation.html#simulaci√≥n-4",
    "title": "Simulaci√≥n matem√°tica con R: Ecuaci√≥n Log√≠stica",
    "section": "Simulaci√≥n 4",
    "text": "Simulaci√≥n 4\n\nEn esta simulaci√≥n se implementan diferentes tasas de crecimiento para ver el resultado en el equilibrio poblacional (en 100 generaciones) con una poblaci√≥n cuyo tama√±o actual es 62% del m√°ximo te√≥rico.\n\n\nC√≥digo# Par√°metross\nr <- seq(from = 0.1, to = 3.6, by = 0.1)\nxt <- 0.62\nt <- 100\nxinit <- xt\n\n# Gr√°ficoo\nr %>%\n  map_df(.x = .,\n         .f = ~ dfGrowth(\n           rate = .,\n           xt = xt,\n           t = t,\n           xinit = xinit\n         )) %>%\n  ggplot(aes(x = t, y = pob, color = rate)) +\n  geom_line(alpha = 0.2) +\n  scale_color_viridis_d() +\n  labs(x = \"Tasa\",\n       y = \"Equilibrio poblacional\",\n       title = \"Estabilidad poblacional\",\n       subtitle = TeX(r'($x_{t+1}= (\\alpha x_t) \\times (1-x_t)$)'))"
  },
  {
    "objectID": "posts/chaos_theory_r/growth-simulation.html#simulaci√≥n-5",
    "href": "posts/chaos_theory_r/growth-simulation.html#simulaci√≥n-5",
    "title": "Simulaci√≥n matem√°tica con R: Ecuaci√≥n Log√≠stica",
    "section": "Simulaci√≥n 5",
    "text": "Simulaci√≥n 5\n\nEn esta simulaci√≥n tomo tasas de crecimiento entre 3.5 y 3.8 (justamente donde aparece el caos) para ver el resultado en el equilibrio poblacional (en 100 generaciones) con una poblaci√≥n cuyo tama√±o actual es 62% del m√°ximo te√≥rico. Nota: para facilitar la visualizaci√≥n de los patrones üòé se presenta de forma interactiva con plotly.\n\n\nC√≥digo# Par√°metross\nr <- seq(from = 3.5, to = 3.8, length = 20)\nxt <- 0.62\nt <- 100\nxinit <- xt\n\n# Gr√°ficoo\nggplotly(\n  r %>%\n  map_df(.x = .,\n         .f = ~ dfGrowth(\n           rate = .,\n           xt = xt,\n           t = t,\n           xinit = xinit\n         )) %>%\n  ggplot(aes(x = t, y = pob, color = rate)) +\n  geom_line(alpha = 0.2) +\n  scale_color_viridis_d() +\n  labs(x = \"Tasa\",\n       y = \"Equilibrio poblacional\",\n       title = \"Estabilidad poblacional\",\n       subtitle = TeX(r'($x_{t+1}= (\\alpha x_t) \\times (1-x_t)$)'))\n)"
  },
  {
    "objectID": "posts/chaos_theory_r/growth-simulation.html#simulaci√≥n-completa",
    "href": "posts/chaos_theory_r/growth-simulation.html#simulaci√≥n-completa",
    "title": "Simulaci√≥n matem√°tica con R: Ecuaci√≥n Log√≠stica",
    "section": "Simulaci√≥n completa",
    "text": "Simulaci√≥n completa\n\n\nrate: 10 mil valores desde 0.1 a 3.8\n\nxt: 0.62\n\nt: 2 mil\n\nxinit: 0.62\n\nNotas:\n\nEstoy asumiendo que la estabilidad poblacional se logra en la mitad del tiempo, que para este ejemplo corresponde a mil.\nComo la simulaci√≥n se demora m√°s de 20 mintuos decid√≠ guardar el archivo con los resultados e importarlos para construir los diagramas de bifurcaci√≥n.\n\n\n\n\nC√≥digo# Par√°metross\nr <- seq(from = 0, to = 4, length = 1000)\nxt <- 0.62\nt <- 2000\nxinit <- xt\nestabPob <- t / 2\n\n# Simulaci√≥nn\nset.seed(2022)\ng <- r %>%\n  map2_df(\n    .x = .,\n    .y = t,\n    .f = ~ prueba(\n      rate = .,\n      xt = xt,\n      t = sample.int(n = .y, size = 1),\n      xinit = xinit\n    ) %>% slice(sample.int(n = .y, size = 1))\n  )\n\n# Exportando resultados simulaci√≥nn\nwrite_csv(x = g, \"data_simulation_rates.csv\")"
  },
  {
    "objectID": "posts/chaos_theory_r/growth-simulation.html#opci√≥n-1",
    "href": "posts/chaos_theory_r/growth-simulation.html#opci√≥n-1",
    "title": "Simulaci√≥n matem√°tica con R: Ecuaci√≥n Log√≠stica",
    "section": "Opci√≥n 1",
    "text": "Opci√≥n 1\n\nC√≥digoresult_simulation <- read_csv(\"data/data_simulation_rates.csv\")\n\nresult_simulation %>% \n  ggplot(aes(x = rate, y = pob)) +\n  geom_point(size = 0.1, color = \"white\")"
  },
  {
    "objectID": "posts/chaos_theory_r/growth-simulation.html#opci√≥n-2",
    "href": "posts/chaos_theory_r/growth-simulation.html#opci√≥n-2",
    "title": "Simulaci√≥n matem√°tica con R: Ecuaci√≥n Log√≠stica",
    "section": "Opci√≥n 2",
    "text": "Opci√≥n 2\n\nC√≥digoresult_simulation %>% \n  ggplot(aes(x = rate, y = pob, color = pob)) +\n  geom_point(size = 0.4) + \n  labs(x = \"Tasa\",\n       y = \"Estabilidad poblacional\",\n       title = \"Diagrama de bifurcaci√≥n\"\",\n       subtitle = TeX(r'($x_{t+1}= (\\alpha x_t) \\times (1-x_t)$)'),\n       caption = TeX(r'(Lnea roja: $ \\alpha \\sim 3.57$)')) +\n  geom_vline(xintercept = 3.57, color = \"red\", size = 0.4) +\n  scale_color_viridis_c() +\n  theme_modern_rc() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/pyr1/Py_R_1.html",
    "href": "posts/pyr1/Py_R_1.html",
    "title": "R + Python: I",
    "section": "",
    "text": "Tener instalado Python\n\nRecomendable instalar Anaconda Navigator\n\nInstalar la biblioteca reticulate desde R."
  },
  {
    "objectID": "posts/pyr1/Py_R_1.html#importando-numpy",
    "href": "posts/pyr1/Py_R_1.html#importando-numpy",
    "title": "R + Python: I",
    "section": "Importando numpy\n",
    "text": "Importando numpy\n\n\nC√≥digonp <- import(\"numpy\")\nnp$argmin(c(2, 1, 3))\n\n[1] 1\n\n\n\nEn esta salida se muestra c√≥mo aplicar la funci√≥n arcgmin de python sobre un vector de R. Devuelve la posici√≥n (√≠ndice) donde se encuentra el valor m√≠nimo del vector dado.\nEs posible acceder a todas las funciones de numpy desde el objeto np con el s√≠mbolo d√≥lar $."
  },
  {
    "objectID": "posts/pyr1/Py_R_1.html#importando-os",
    "href": "posts/pyr1/Py_R_1.html#importando-os",
    "title": "R + Python: I",
    "section": "Importando os\n",
    "text": "Importando os\n\n\nC√≥digoos <- import(\"os\")\nos$getcwd()\n\n[1] \"D:\\\\Otros\\\\Github\\\\web-edimer\\\\web-edimer.github.io\\\\posts\\\\pyr1\""
  },
  {
    "objectID": "posts/pyr1/Py_R_1.html#tupla-desde-r",
    "href": "posts/pyr1/Py_R_1.html#tupla-desde-r",
    "title": "R + Python: I",
    "section": "Tupla desde R\n",
    "text": "Tupla desde R\n\n\nCreando una tupla y obteniendo su clase:\n\n\nC√≥digotupla1 <- tuple(c(1, 2, 3, \"A\"))\ntupla1\n## (['1', '2', '3', 'A'],)\n\n# Clase en python\nclass(tupla1)\n## [1] \"python.builtin.tuple\"  \"python.builtin.object\"\n\n\n\nCoercionar el objeto tupla1 de clase tuple en python directamente a R:\n\n\nC√≥digotupla1_r <- py_to_r(tupla1)\ntupla1_r\n## [[1]]\n## [1] \"1\" \"2\" \"3\" \"A\"\n\n# Clase en R\nclass(tupla1_r)\n## [1] \"list\""
  },
  {
    "objectID": "posts/pyr1/Py_R_1.html#diccionario-desde-r",
    "href": "posts/pyr1/Py_R_1.html#diccionario-desde-r",
    "title": "R + Python: I",
    "section": "Diccionario desde R\n",
    "text": "Diccionario desde R\n\n\nC√≥digo# Objeto\ndict1 <- dict(x = \"Hola\", y = 3.5, z = 1L)\ndict1\n## {'x': 'Hola', 'y': 3.5, 'z': 1}\n\n# Clase\nclass(dict1)\n## [1] \"python.builtin.dict\"   \"python.builtin.object\"\n\n# Nombres\nnames(dict1)\n## [1] \"x\" \"y\" \"z\"\n\n# Atributos\nattributes(dict1)\n## $class\n## [1] \"python.builtin.dict\"   \"python.builtin.object\"\n\n# Coerci√≥n a objeto RR\ndict1_r <- py_to_r(dict1)\ndict1_r\n## $x\n## [1] \"Hola\"\n## \n## $y\n## [1] 3.5\n## \n## $z\n## [1] 1\n\n# Clase en R\nclass(dict1_r)\n## [1] \"list\""
  },
  {
    "objectID": "posts/pyr1/Py_R_1.html#tupla-en-python",
    "href": "posts/pyr1/Py_R_1.html#tupla-en-python",
    "title": "R + Python: I",
    "section": "Tupla en python\n",
    "text": "Tupla en python\n\n\nCreando tupla en python:\n\n\nC√≥digo# Creando tupla\naltura = (1.65, 1.72, 1.56, 1.84, 1.92)\naltura\n\n(1.65, 1.72, 1.56, 1.84, 1.92)\n\n\n\nC√≥digo# Otra tupla\npeso = (67, 75, 67, 78, 85)\npeso\n\n(67, 75, 67, 78, 85)\n\n\n\nC√≥digo# Tipo (clase) de objetos\ntype(altura)\n\n<class 'tuple'>\n\nC√≥digotype(peso)\n\n<class 'tuple'>\n\n\n\nLlamando la tupla desde R:\n\n\nC√≥digoclass(py$altura)\n## [1] \"list\"\nplot(x = py$altura, y = py$peso, pch = 19, cex = 2)"
  },
  {
    "objectID": "posts/pyr1/Py_R_1.html#tipos-de-objetos-en-ambos-lenguajes",
    "href": "posts/pyr1/Py_R_1.html#tipos-de-objetos-en-ambos-lenguajes",
    "title": "R + Python: I",
    "section": "Tipos de objetos en ambos lenguajes",
    "text": "Tipos de objetos en ambos lenguajes"
  },
  {
    "objectID": "posts/pyr1/Py_R_1.html#instalando-pandas",
    "href": "posts/pyr1/Py_R_1.html#instalando-pandas",
    "title": "R + Python: I",
    "section": "Instalando pandas\n",
    "text": "Instalando pandas\n\n\nC√≥digopy_install(\"pandas\")"
  },
  {
    "objectID": "posts/pyr1/Py_R_1.html#importando-pandas-y-leyendo-archivo-.csv",
    "href": "posts/pyr1/Py_R_1.html#importando-pandas-y-leyendo-archivo-.csv",
    "title": "R + Python: I",
    "section": "Importando pandas y leyendo archivo .csv\n",
    "text": "Importando pandas y leyendo archivo .csv\n\n\nC√≥digoimport pandas as pd\niris_py = pd.read_csv(\"Iris.csv\")\niris_py\n\n     Sepal.Length  Sepal.Width  Petal.Length  Petal.Width    Species\n0             5.1          3.5           1.4          0.2     setosa\n1             4.9          3.0           1.4          0.2     setosa\n2             4.7          3.2           1.3          0.2     setosa\n3             4.6          3.1           1.5          0.2     setosa\n4             5.0          3.6           1.4          0.2     setosa\n..            ...          ...           ...          ...        ...\n145           6.7          3.0           5.2          2.3  virginica\n146           6.3          2.5           5.0          1.9  virginica\n147           6.5          3.0           5.2          2.0  virginica\n148           6.2          3.4           5.4          2.3  virginica\n149           5.9          3.0           5.1          1.8  virginica\n\n[150 rows x 5 columns]\n\n\n\nC√≥digotype(iris_py)\n\n<class 'pandas.core.frame.DataFrame'>\n\n\n\nEstad√≠sticos descriptivos:\n\n\nC√≥digoiris_py.describe()\n\n       Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\ncount    150.000000   150.000000    150.000000   150.000000\nmean       5.843333     3.057333      3.758000     1.199333\nstd        0.828066     0.435866      1.765298     0.762238\nmin        4.300000     2.000000      1.000000     0.100000\n25%        5.100000     2.800000      1.600000     0.300000\n50%        5.800000     3.000000      4.350000     1.300000\n75%        6.400000     3.300000      5.100000     1.800000\nmax        7.900000     4.400000      6.900000     2.500000\n\n\n\nSeleccionando variables por nombre:\n\n\nC√≥digoiris_py[[\"Sepal.Length\", \"Sepal.Width\"]]\n\n     Sepal.Length  Sepal.Width\n0             5.1          3.5\n1             4.9          3.0\n2             4.7          3.2\n3             4.6          3.1\n4             5.0          3.6\n..            ...          ...\n145           6.7          3.0\n146           6.3          2.5\n147           6.5          3.0\n148           6.2          3.4\n149           5.9          3.0\n\n[150 rows x 2 columns]\n\n\n\nFiltrando datos:\n\n\nC√≥digofiltro = iris_py[\"Sepal.Width\"] <= 2.2\niris_py[filtro]\n\n     Sepal.Length  Sepal.Width  Petal.Length  Petal.Width     Species\n60            5.0          2.0           3.5          1.0  versicolor\n62            6.0          2.2           4.0          1.0  versicolor\n68            6.2          2.2           4.5          1.5  versicolor\n119           6.0          2.2           5.0          1.5   virginica\n\n\n\n\nDataframe como array:\n\n\nC√≥digoiris_py.values\n\narray([[5.1, 3.5, 1.4, 0.2, 'setosa'],\n       [4.9, 3.0, 1.4, 0.2, 'setosa'],\n       [4.7, 3.2, 1.3, 0.2, 'setosa'],\n       [4.6, 3.1, 1.5, 0.2, 'setosa'],\n       [5.0, 3.6, 1.4, 0.2, 'setosa'],\n       [5.4, 3.9, 1.7, 0.4, 'setosa'],\n       [4.6, 3.4, 1.4, 0.3, 'setosa'],\n       [5.0, 3.4, 1.5, 0.2, 'setosa'],\n       [4.4, 2.9, 1.4, 0.2, 'setosa'],\n       [4.9, 3.1, 1.5, 0.1, 'setosa'],\n       [5.4, 3.7, 1.5, 0.2, 'setosa'],\n       [4.8, 3.4, 1.6, 0.2, 'setosa'],\n       [4.8, 3.0, 1.4, 0.1, 'setosa'],\n       [4.3, 3.0, 1.1, 0.1, 'setosa'],\n       [5.8, 4.0, 1.2, 0.2, 'setosa'],\n       [5.7, 4.4, 1.5, 0.4, 'setosa'],\n       [5.4, 3.9, 1.3, 0.4, 'setosa'],\n       [5.1, 3.5, 1.4, 0.3, 'setosa'],\n       [5.7, 3.8, 1.7, 0.3, 'setosa'],\n       [5.1, 3.8, 1.5, 0.3, 'setosa'],\n       [5.4, 3.4, 1.7, 0.2, 'setosa'],\n       [5.1, 3.7, 1.5, 0.4, 'setosa'],\n       [4.6, 3.6, 1.0, 0.2, 'setosa'],\n       [5.1, 3.3, 1.7, 0.5, 'setosa'],\n       [4.8, 3.4, 1.9, 0.2, 'setosa'],\n       [5.0, 3.0, 1.6, 0.2, 'setosa'],\n       [5.0, 3.4, 1.6, 0.4, 'setosa'],\n       [5.2, 3.5, 1.5, 0.2, 'setosa'],\n       [5.2, 3.4, 1.4, 0.2, 'setosa'],\n       [4.7, 3.2, 1.6, 0.2, 'setosa'],\n       [4.8, 3.1, 1.6, 0.2, 'setosa'],\n       [5.4, 3.4, 1.5, 0.4, 'setosa'],\n       [5.2, 4.1, 1.5, 0.1, 'setosa'],\n       [5.5, 4.2, 1.4, 0.2, 'setosa'],\n       [4.9, 3.1, 1.5, 0.2, 'setosa'],\n       [5.0, 3.2, 1.2, 0.2, 'setosa'],\n       [5.5, 3.5, 1.3, 0.2, 'setosa'],\n       [4.9, 3.6, 1.4, 0.1, 'setosa'],\n       [4.4, 3.0, 1.3, 0.2, 'setosa'],\n       [5.1, 3.4, 1.5, 0.2, 'setosa'],\n       [5.0, 3.5, 1.3, 0.3, 'setosa'],\n       [4.5, 2.3, 1.3, 0.3, 'setosa'],\n       [4.4, 3.2, 1.3, 0.2, 'setosa'],\n       [5.0, 3.5, 1.6, 0.6, 'setosa'],\n       [5.1, 3.8, 1.9, 0.4, 'setosa'],\n       [4.8, 3.0, 1.4, 0.3, 'setosa'],\n       [5.1, 3.8, 1.6, 0.2, 'setosa'],\n       [4.6, 3.2, 1.4, 0.2, 'setosa'],\n       [5.3, 3.7, 1.5, 0.2, 'setosa'],\n       [5.0, 3.3, 1.4, 0.2, 'setosa'],\n       [7.0, 3.2, 4.7, 1.4, 'versicolor'],\n       [6.4, 3.2, 4.5, 1.5, 'versicolor'],\n       [6.9, 3.1, 4.9, 1.5, 'versicolor'],\n       [5.5, 2.3, 4.0, 1.3, 'versicolor'],\n       [6.5, 2.8, 4.6, 1.5, 'versicolor'],\n       [5.7, 2.8, 4.5, 1.3, 'versicolor'],\n       [6.3, 3.3, 4.7, 1.6, 'versicolor'],\n       [4.9, 2.4, 3.3, 1.0, 'versicolor'],\n       [6.6, 2.9, 4.6, 1.3, 'versicolor'],\n       [5.2, 2.7, 3.9, 1.4, 'versicolor'],\n       [5.0, 2.0, 3.5, 1.0, 'versicolor'],\n       [5.9, 3.0, 4.2, 1.5, 'versicolor'],\n       [6.0, 2.2, 4.0, 1.0, 'versicolor'],\n       [6.1, 2.9, 4.7, 1.4, 'versicolor'],\n       [5.6, 2.9, 3.6, 1.3, 'versicolor'],\n       [6.7, 3.1, 4.4, 1.4, 'versicolor'],\n       [5.6, 3.0, 4.5, 1.5, 'versicolor'],\n       [5.8, 2.7, 4.1, 1.0, 'versicolor'],\n       [6.2, 2.2, 4.5, 1.5, 'versicolor'],\n       [5.6, 2.5, 3.9, 1.1, 'versicolor'],\n       [5.9, 3.2, 4.8, 1.8, 'versicolor'],\n       [6.1, 2.8, 4.0, 1.3, 'versicolor'],\n       [6.3, 2.5, 4.9, 1.5, 'versicolor'],\n       [6.1, 2.8, 4.7, 1.2, 'versicolor'],\n       [6.4, 2.9, 4.3, 1.3, 'versicolor'],\n       [6.6, 3.0, 4.4, 1.4, 'versicolor'],\n       [6.8, 2.8, 4.8, 1.4, 'versicolor'],\n       [6.7, 3.0, 5.0, 1.7, 'versicolor'],\n       [6.0, 2.9, 4.5, 1.5, 'versicolor'],\n       [5.7, 2.6, 3.5, 1.0, 'versicolor'],\n       [5.5, 2.4, 3.8, 1.1, 'versicolor'],\n       [5.5, 2.4, 3.7, 1.0, 'versicolor'],\n       [5.8, 2.7, 3.9, 1.2, 'versicolor'],\n       [6.0, 2.7, 5.1, 1.6, 'versicolor'],\n       [5.4, 3.0, 4.5, 1.5, 'versicolor'],\n       [6.0, 3.4, 4.5, 1.6, 'versicolor'],\n       [6.7, 3.1, 4.7, 1.5, 'versicolor'],\n       [6.3, 2.3, 4.4, 1.3, 'versicolor'],\n       [5.6, 3.0, 4.1, 1.3, 'versicolor'],\n       [5.5, 2.5, 4.0, 1.3, 'versicolor'],\n       [5.5, 2.6, 4.4, 1.2, 'versicolor'],\n       [6.1, 3.0, 4.6, 1.4, 'versicolor'],\n       [5.8, 2.6, 4.0, 1.2, 'versicolor'],\n       [5.0, 2.3, 3.3, 1.0, 'versicolor'],\n       [5.6, 2.7, 4.2, 1.3, 'versicolor'],\n       [5.7, 3.0, 4.2, 1.2, 'versicolor'],\n       [5.7, 2.9, 4.2, 1.3, 'versicolor'],\n       [6.2, 2.9, 4.3, 1.3, 'versicolor'],\n       [5.1, 2.5, 3.0, 1.1, 'versicolor'],\n       [5.7, 2.8, 4.1, 1.3, 'versicolor'],\n       [6.3, 3.3, 6.0, 2.5, 'virginica'],\n       [5.8, 2.7, 5.1, 1.9, 'virginica'],\n       [7.1, 3.0, 5.9, 2.1, 'virginica'],\n       [6.3, 2.9, 5.6, 1.8, 'virginica'],\n       [6.5, 3.0, 5.8, 2.2, 'virginica'],\n       [7.6, 3.0, 6.6, 2.1, 'virginica'],\n       [4.9, 2.5, 4.5, 1.7, 'virginica'],\n       [7.3, 2.9, 6.3, 1.8, 'virginica'],\n       [6.7, 2.5, 5.8, 1.8, 'virginica'],\n       [7.2, 3.6, 6.1, 2.5, 'virginica'],\n       [6.5, 3.2, 5.1, 2.0, 'virginica'],\n       [6.4, 2.7, 5.3, 1.9, 'virginica'],\n       [6.8, 3.0, 5.5, 2.1, 'virginica'],\n       [5.7, 2.5, 5.0, 2.0, 'virginica'],\n       [5.8, 2.8, 5.1, 2.4, 'virginica'],\n       [6.4, 3.2, 5.3, 2.3, 'virginica'],\n       [6.5, 3.0, 5.5, 1.8, 'virginica'],\n       [7.7, 3.8, 6.7, 2.2, 'virginica'],\n       [7.7, 2.6, 6.9, 2.3, 'virginica'],\n       [6.0, 2.2, 5.0, 1.5, 'virginica'],\n       [6.9, 3.2, 5.7, 2.3, 'virginica'],\n       [5.6, 2.8, 4.9, 2.0, 'virginica'],\n       [7.7, 2.8, 6.7, 2.0, 'virginica'],\n       [6.3, 2.7, 4.9, 1.8, 'virginica'],\n       [6.7, 3.3, 5.7, 2.1, 'virginica'],\n       [7.2, 3.2, 6.0, 1.8, 'virginica'],\n       [6.2, 2.8, 4.8, 1.8, 'virginica'],\n       [6.1, 3.0, 4.9, 1.8, 'virginica'],\n       [6.4, 2.8, 5.6, 2.1, 'virginica'],\n       [7.2, 3.0, 5.8, 1.6, 'virginica'],\n       [7.4, 2.8, 6.1, 1.9, 'virginica'],\n       [7.9, 3.8, 6.4, 2.0, 'virginica'],\n       [6.4, 2.8, 5.6, 2.2, 'virginica'],\n       [6.3, 2.8, 5.1, 1.5, 'virginica'],\n       [6.1, 2.6, 5.6, 1.4, 'virginica'],\n       [7.7, 3.0, 6.1, 2.3, 'virginica'],\n       [6.3, 3.4, 5.6, 2.4, 'virginica'],\n       [6.4, 3.1, 5.5, 1.8, 'virginica'],\n       [6.0, 3.0, 4.8, 1.8, 'virginica'],\n       [6.9, 3.1, 5.4, 2.1, 'virginica'],\n       [6.7, 3.1, 5.6, 2.4, 'virginica'],\n       [6.9, 3.1, 5.1, 2.3, 'virginica'],\n       [5.8, 2.7, 5.1, 1.9, 'virginica'],\n       [6.8, 3.2, 5.9, 2.3, 'virginica'],\n       [6.7, 3.3, 5.7, 2.5, 'virginica'],\n       [6.7, 3.0, 5.2, 2.3, 'virginica'],\n       [6.3, 2.5, 5.0, 1.9, 'virginica'],\n       [6.5, 3.0, 5.2, 2.0, 'virginica'],\n       [6.2, 3.4, 5.4, 2.3, 'virginica'],\n       [5.9, 3.0, 5.1, 1.8, 'virginica']], dtype=object)"
  },
  {
    "objectID": "posts/pyr1/Py_R_1.html#importando-tensorflow-desde-python",
    "href": "posts/pyr1/Py_R_1.html#importando-tensorflow-desde-python",
    "title": "R + Python: I",
    "section": "Importando tensorflow desde python",
    "text": "Importando tensorflow desde python\n\nC√≥digoimport tensorflow as tf\n\n\n\nFunciones desde tf:"
  },
  {
    "objectID": "posts/pyr1/Py_R_1.html#ejemplo-1",
    "href": "posts/pyr1/Py_R_1.html#ejemplo-1",
    "title": "R + Python: I",
    "section": "Ejemplo 1",
    "text": "Ejemplo 1\n\nC√≥digoimport matplotlib.pyplot as plt\nx = np.arange(0, 20)\ny = x**2\ng1 = plt.plot(x, y, \"g--\")\ng1 = plt.title(\"X vs Y\")\ng1 = plt.xlabel(\"Eje x\")\ng1 = plt.ylabel(\"Eje Y\")  \ng1"
  },
  {
    "objectID": "posts/pyr1/Py_R_1.html#ejemplo-2",
    "href": "posts/pyr1/Py_R_1.html#ejemplo-2",
    "title": "R + Python: I",
    "section": "Ejemplo 2",
    "text": "Ejemplo 2\n\nC√≥digow = np.arange(0, 50).reshape(5, 10)\nw\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n       [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n       [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]])\n\n\n\nC√≥digog2 = plt.imshow(w)\ng2 = plt.colorbar()\nplt.show()"
  },
  {
    "objectID": "posts/pyr1/Py_R_1.html#ejemplo-3",
    "href": "posts/pyr1/Py_R_1.html#ejemplo-3",
    "title": "R + Python: I",
    "section": "Ejemplo 3",
    "text": "Ejemplo 3\n\nGr√°fico desde un Dataframe:\n\n\nC√≥digog3 = iris_py.plot(x = \"Sepal.Length\", y = \"Sepal.Width\", kind = \"scatter\",\n                  color = \"red\")                 \ng3"
  },
  {
    "objectID": "posts/pyr1/Py_R_1.html#importando-m√≥dulos-de-python-1",
    "href": "posts/pyr1/Py_R_1.html#importando-m√≥dulos-de-python-1",
    "title": "R + Python: I",
    "section": "Importando m√≥dulos de python\n",
    "text": "Importando m√≥dulos de python\n\n\nC√≥digoimport pandas as pd\nimport sklearn\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.model_selection import train_test_split \nfrom sklearn import metrics"
  },
  {
    "objectID": "posts/pyr1/Py_R_1.html#cargando-datos",
    "href": "posts/pyr1/Py_R_1.html#cargando-datos",
    "title": "R + Python: I",
    "section": "Cargando datos",
    "text": "Cargando datos\n\nC√≥digocol_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree',\n             'age', 'label']\n# load dataset\npima = pd.read_csv(\"diabetes.csv\", header=None, names=col_names)\npima = pima[1:]\npima.head()\n\n  pregnant glucose  bp skin insulin   bmi pedigree age label\n1        6     148  72   35       0  33.6    0.627  50     1\n2        1      85  66   29       0  26.6    0.351  31     0\n3        8     183  64    0       0  23.3    0.672  32     1\n4        1      89  66   23      94  28.1    0.167  21     0\n5        0     137  40   35     168  43.1    2.288  33     1"
  },
  {
    "objectID": "posts/pyr1/Py_R_1.html#selecci√≥n-de-caracter√≠sticas",
    "href": "posts/pyr1/Py_R_1.html#selecci√≥n-de-caracter√≠sticas",
    "title": "R + Python: I",
    "section": "Selecci√≥n de caracter√≠sticas",
    "text": "Selecci√≥n de caracter√≠sticas\n\nC√≥digo# Fraccionando la base de datos en predictoras (X) y respuesta (Y)\nfeature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']\nX = pima[feature_cols] \ny = pima.label"
  },
  {
    "objectID": "posts/pyr1/Py_R_1.html#train---test",
    "href": "posts/pyr1/Py_R_1.html#train---test",
    "title": "R + Python: I",
    "section": "Train - Test",
    "text": "Train - Test\n\nC√≥digoX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
  },
  {
    "objectID": "posts/pyr1/Py_R_1.html#construyendo-modelo",
    "href": "posts/pyr1/Py_R_1.html#construyendo-modelo",
    "title": "R + Python: I",
    "section": "Construyendo Modelo",
    "text": "Construyendo Modelo\n\nC√≥digo# Clasificador\nclf = DecisionTreeClassifier()\n\n# Clasificador en train --> Entrenando modelo\nclf_fit = clf.fit(X = X_train, y = y_train)"
  },
  {
    "objectID": "posts/pyr1/Py_R_1.html#evaluaci√≥n-del-modelo",
    "href": "posts/pyr1/Py_R_1.html#evaluaci√≥n-del-modelo",
    "title": "R + Python: I",
    "section": "Evaluaci√≥n del modelo",
    "text": "Evaluaci√≥n del modelo\n\nC√≥digo# Predicciones\ny_pred = clf_fit.predict(X_test)\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n\nAccuracy: 0.6753246753246753"
  },
  {
    "objectID": "posts/tree_decision/TreeD_R.html#generalidades",
    "href": "posts/tree_decision/TreeD_R.html#generalidades",
    "title": "√Årbol de clasificaci√≥n con R",
    "section": "Generalidades",
    "text": "Generalidades\n\n\n\n\nLos √°rboles de decisi√≥n se pueden utilizar para problemas de regresi√≥n y clasificaci√≥n.\nSe pueden definir como una estructura jer√°rquica que busca particionar el espacio de caracter√≠sticas e identificar subconjuntos representativos. Desde la parte superior a inferior cada √°rbol tiene nodo ra√≠z, nodos de decisi√≥n o internos y nodos hojas o terminales, los dos primeros se generan con base en reglas binarias. Mayor grado de pureza es la recompensa que busca el algoritmo al particionar el espacio inicial en subregiones, en ese orden de ideas el objetivo siempre ser√° particionar los datos en nodos que sean lo m√°s puros posible, sin embargo, matem√°ticamente es m√°s f√°cil medir la impureza de una regi√≥n espec√≠fica, proporcionando una idea de qu√© tan heterog√©neas son las clases en ese nodo; una m√©trica de uso com√∫n en problemas de clasificaci√≥n para medir la impureza es el √≠ndice GINI, donde valores bajos indican mayor grado de pureza. Adem√°s del √≠ndice GINI tambi√©n es posible utilizar otras m√©tricas como la ganancia de informaci√≥n o la entrop√≠a.\n\nVentajas:\n\nF√°cil de interpretar (sujeto a la profundidad).\nNo requieren estandarizaci√≥n o normalizaci√≥n de variables predictoras num√©ricas.\nPermiten manipular variables categ√≥ricas sin necesidad de aplicar codificaciones tipo one-hot o variables dummy.\nRequieren poco preprocesamiento de datos.\nPermiten valores ausentes (NA).\nPermite relaciones no lineales.\n\n\n\nDesventajas:\n\nSi no se controla adecuadamente la profundidad del √°rbol existe alta probabilidad de incurrir en sobreajuste (overfitting).\n\nAlta varianza, peque√±os cambios en los datos pueden arrojar resultados muy diferentes.\n\n\n\nHiperpar√°metros: aunque ejecutar la funci√≥n rpart() con valores predeterminados puede ser una buena estrategia para iniciar, siempre estaremos interesados en ajustar determinados par√°metros que nos permitan obtener mejor rendimiento predictivo. La funci√≥n rpart.control() permite controlar manualmente otras opciones. Dentro los hiperpar√°metros m√°s importantes en √°rboles de decisi√≥n est√°n los siguientes:\n\nM√≠nimo n√∫mero de observaciones para generar una partici√≥n. En la biblioteca rpart lleva el nombre de minsplit y su valor por defecto es 20.\nM√°xima profundiad del √°rbol. En la biblioteca rpart lleva el nombre de maxdepth y su valor predeterminado es 30. Este par√°metro es de alta relevancia para evitar el sobreajuste.\nPar√°metro de complejidad. En la biblioteca rpart lleva el nombre de cp y su valor por defecto es 0.01. Este par√°metro sirve al prop√≥sito de penalizar y contolar el tama√±o del √°rbol, valores bajos indican √°rboles de mayor complejidad, es decir, mayor n√∫mero de divisiones. La funci√≥n rpart() internamente ejecuta validaci√≥n cruzada 10 veces para estimar el valor √≥ptimo de cp, es posible acceder a dicho resultado a trav√©s de la funci√≥n plotcp() que permitir√° facilmente determinar el valor adecuado para este par√°metro. Cuando se tiene el valor √≥ptimo de cp ser√° posible ‚Äúpodar‚Äù el √°rbol para que el modelo sea optimizado, dicho resultado es posible a trav√©s de la funci√≥n prune()."
  },
  {
    "objectID": "posts/tree_decision/TreeD_R.html#train---test",
    "href": "posts/tree_decision/TreeD_R.html#train---test",
    "title": "√Årbol de clasificaci√≥n con R",
    "section": "Train - Test",
    "text": "Train - Test\n\nPara entrenar el modelo inicialmente fracciono los datos en train y test con proporciones de 70 y 30%, respectivamente. Este proceso aunque es posible hacerlo manualmente con la funci√≥n sample(), la funci√≥n createDataPartition() del paquete caret agiliza el procedimiento. Para garantizar replicabilidad en los resultados se agrega la semilla.\n\n\nC√≥digoset.seed(1992)\nidx <- createDataPartition(y = data$class, times = 1, p = 0.70, list = FALSE)\ndataTrain <- data[idx, ]\ndataTest <- data[-idx, ]"
  },
  {
    "objectID": "posts/tree_decision/TreeD_R.html#rpart-default",
    "href": "posts/tree_decision/TreeD_R.html#rpart-default",
    "title": "√Årbol de clasificaci√≥n con R",
    "section": "\nrpart default\n",
    "text": "rpart default\n\n\n\nAjuste del modelo: en este caso se utilizan todas las variables predictoras para entrenar el modelo. Al imprimir el objeto que contiene el modelo podemos observar el conjunto de reglas que dan como resultado la estructura final del √°rbol. El m√©todo igualado a method = \"class\" indica que es un problema de clasificaci√≥n, si fuese un problema de regresi√≥n el argumento tomar√≠a el valor de method = \"anova\", aunque tambi√©n permite otras opciones (consulte ?rpart o help(\"rpart\")).\n\n\nC√≥digomodArbol0 <- rpart(class ~ ., data = dataTrain, method = \"class\")\nmodArbol0\n\nn= 700 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 700 210 good (0.3000000 0.7000000)  \n    2) checking_status=<0,0<=X<200 385 168 good (0.4363636 0.5636364)  \n      4) credit_history=all paid,no credits/all paid 51  14 bad (0.7254902 0.2745098)  \n        8) savings_status=<100,100<=X<500 42   8 bad (0.8095238 0.1904762) *\n        9) savings_status=500<=X<1000,no known savings 9   3 good (0.3333333 0.6666667) *\n      5) credit_history=critical/other existing credit,delayed previously,existing paid 334 131 good (0.3922156 0.6077844)  \n       10) duration>=27.5 77  30 bad (0.6103896 0.3896104)  \n         20) purpose=domestic appliance,education,furniture/equipment,new car 39  10 bad (0.7435897 0.2564103) *\n         21) purpose=business,other,radio/tv,repairs,used car 38  18 good (0.4736842 0.5263158)  \n           42) checking_status=<0 19   7 bad (0.6315789 0.3684211) *\n           43) checking_status=0<=X<200 19   6 good (0.3157895 0.6842105) *\n       11) duration< 27.5 257  84 good (0.3268482 0.6731518)  \n         22) purpose=domestic appliance,education,new car,retraining 83  41 good (0.4939759 0.5060241)  \n           44) age< 35.5 42  13 bad (0.6904762 0.3095238)  \n             88) credit_amount< 1392 20   1 bad (0.9500000 0.0500000) *\n             89) credit_amount>=1392 22  10 good (0.4545455 0.5454545)  \n              178) property_magnitude=life insurance,real estate 13   4 bad (0.6923077 0.3076923) *\n              179) property_magnitude=car,no known property 9   1 good (0.1111111 0.8888889) *\n           45) age>=35.5 41  12 good (0.2926829 0.7073171) *\n         23) purpose=business,furniture/equipment,other,radio/tv,repairs,used car 174  43 good (0.2471264 0.7528736) *\n    3) checking_status=>=200,no checking 315  42 good (0.1333333 0.8666667) *\n\n\n\n\nGr√°fico del modelo: la variable m√°s importante y que da origen al nodo ra√≠z es checking_status, que hace referencia al estado de la cuenta corriente. El historial crediticio, la duraci√≥n del tiempo para pagar el cr√©dito, el estado de la cuenta de ahorros y el prop√≥sito del cr√©dito, tambi√©n son factores determinantes. No tener suficiente capital en cualquiera de las dos cuentas, tener mal historial crediticio y adem√°s solicitar per√≠odos de pago de alta duraci√≥n, pueden ser caracter√≠sticas no deseables a la hora de solicitar un cr√©dito.\n\n\nC√≥digorpart.plot(modArbol0)\n\n\n\n\n\n\n\n\n\nMatriz de confusi√≥n: el modelo por default tiene precisi√≥n promedio de 0.7167, con dificultades para clasificar de forma correcta los ‚Äúmalos‚Äù, es decir, que tiene baja especificidad.\n\n\nC√≥digopredichos_mod0 <- predict(object = modArbol0, newdata = dataTest, type = \"class\")\nconfusionMatrix(data = predichos_mod0, reference = as.factor(dataTest$class),\n                positive = \"good\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction bad good\n      bad   31   26\n      good  59  184\n                                        \n               Accuracy : 0.7167        \n                 95% CI : (0.662, 0.767)\n    No Information Rate : 0.7           \n    P-Value [Acc > NIR] : 0.2873596     \n                                        \n                  Kappa : 0.2465        \n                                        \n Mcnemar's Test P-Value : 0.0005187     \n                                        \n            Sensitivity : 0.8762        \n            Specificity : 0.3444        \n         Pos Pred Value : 0.7572        \n         Neg Pred Value : 0.5439        \n             Prevalence : 0.7000        \n         Detection Rate : 0.6133        \n   Detection Prevalence : 0.8100        \n      Balanced Accuracy : 0.6103        \n                                        \n       'Positive' Class : good          \n                                        \n\n\n\n√Årea bajo la curva:\n\n\nC√≥digo# Probabilidades predichas para la clase \"good\"\npred0 <- as.data.frame(predict(object = modArbol0,\n                               newdata = dataTest, type = \"prob\"))$good\n\n# Transformando respuesta a entero. A la clase \"good\" le agrego 1 y \n# a la clase \"bad\" le agrego 0.\ntarget <- as.integer(as.factor(dataTest$class)) - 1\n\n# AUC\nMetrics::auc(actual = target, predicted = pred0)\n\n[1] 0.7034392\n\n\n\n\nCurva ROC: la funci√≥n con la que obtengo el siguiente gr√°fico puede ser encontrada en mi Github.\n\n\n\nC√≥digo# Cargando funci√≥nn\nsource(\"functions/myROC.R\")\n\n# Ver funci√≥n myROC() al final en material complementario     \nmyROC(predichos = pred0, reales = target)\n\n\n\n\n\n\n\n\nPar√°metro de complejidad (CP):\n\n\nC√≥digoplotcp(modArbol0)\n\n\n\n\n\n\n\n\n\n‚ÄúPodando‚Äù el √°rbol: se elige el valor de cp = 0.025 por mostrar mejores resultados (bajo error).\n\n\nC√≥digomodArbol0_prune <- prune(tree = modArbol0, cp = 0.025)\nmodArbol0_prune\n\nn= 700 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 700 210 good (0.3000000 0.7000000)  \n   2) checking_status=<0,0<=X<200 385 168 good (0.4363636 0.5636364)  \n     4) credit_history=all paid,no credits/all paid 51  14 bad (0.7254902 0.2745098) *\n     5) credit_history=critical/other existing credit,delayed previously,existing paid 334 131 good (0.3922156 0.6077844)  \n      10) duration>=27.5 77  30 bad (0.6103896 0.3896104) *\n      11) duration< 27.5 257  84 good (0.3268482 0.6731518)  \n        22) purpose=domestic appliance,education,new car,retraining 83  41 good (0.4939759 0.5060241)  \n          44) age< 35.5 42  13 bad (0.6904762 0.3095238) *\n          45) age>=35.5 41  12 good (0.2926829 0.7073171) *\n        23) purpose=business,furniture/equipment,other,radio/tv,repairs,used car 174  43 good (0.2471264 0.7528736) *\n   3) checking_status=>=200,no checking 315  42 good (0.1333333 0.8666667) *\n\n\n\nGr√°fico de √°rbol con ‚Äúpoda‚Äù:\n\n\nC√≥digorpart.plot(modArbol0_prune)\n\n\n\n\n\n\n\n\n\nMatriz de confusi√≥n √°rbol con ‚Äúpoda‚Äù: respecto al √°rbol sin podar, la diferencia en precisi√≥n es muy peque√±a (<0.01), sin embargo, la especificidad se aumenta de 0.3444 a 0.4556 con la ‚Äúpoda‚Äù, aunque la sensitividad haya reducido de 0.8762 a 0.8381.\n\n\nC√≥digopredichos_mod0_prune <- predict(object = modArbol0_prune, newdata = dataTest, type = \"class\")\nconfusionMatrix(data = predichos_mod0_prune, reference = as.factor(dataTest$class),\n                positive = \"good\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction bad good\n      bad   41   34\n      good  49  176\n                                         \n               Accuracy : 0.7233         \n                 95% CI : (0.669, 0.7732)\n    No Information Rate : 0.7            \n    P-Value [Acc > NIR] : 0.2072         \n                                         \n                  Kappa : 0.3083         \n                                         \n Mcnemar's Test P-Value : 0.1244         \n                                         \n            Sensitivity : 0.8381         \n            Specificity : 0.4556         \n         Pos Pred Value : 0.7822         \n         Neg Pred Value : 0.5467         \n             Prevalence : 0.7000         \n         Detection Rate : 0.5867         \n   Detection Prevalence : 0.7500         \n      Balanced Accuracy : 0.6468         \n                                         \n       'Positive' Class : good           \n                                         \n\n\n\n√Årea bajo la curva de √°rbol con ‚Äúpoda‚Äù:\n\n\nC√≥digo# Probabilidades predichas para la clase \"good\"\npred0_prune <- as.data.frame(predict(object = modArbol0_prune,\n                                     newdata = dataTest, type = \"prob\"))$good\n\n# AUC\nMetrics::auc(actual = target, predicted = pred0_prune)\n\n[1] 0.7203175\n\n\n\nCurva ROC:\n\n\nC√≥digo# Ver funci√≥n myROC() al final en material complementario     \nmyROC(predichos = pred0_prune, reales = target)"
  },
  {
    "objectID": "posts/tree_decision/TreeD_R.html#tuning-con-caret",
    "href": "posts/tree_decision/TreeD_R.html#tuning-con-caret",
    "title": "√Årbol de clasificaci√≥n con R",
    "section": "Tuning con caret",
    "text": "Tuning con caret\n\nA diferencia de los par√°metros que se ‚Äúaprenden‚Äù duarante el entrenamiento del modelo, los hipepar√°metros se definen previo al ajuste del mismo.\nEl ajuste de hiperpar√°metros se constituye como parte fundamental de la optimizaci√≥n del modelo.\nLa biblioteca caret proporciona un marco de trabajo unificado para entrenar y validar modelos de machine learning. En este caso, con caret podremos ajustar dos de los tres hiperpar√°metros mencionados anteriormente, el par√°metro de complejidad y la profundidad del √°rbol. Las funciones trainControl() y train de la biblioteca caret facilitan el proceso.\n\n\ntrainControl(): permite establecer la estrategia de validaci√≥n, por ejemplo validaci√≥n cruzada k-fold, validaci√≥n cruzada repetida, bootstrapping, entre otras. Desde esta misma funci√≥n tambi√©n es posible determinar el m√©todo de b√∫squeda de hiperpar√°metros, que puede ser aleatoria o cuadr√≠cula (grid). En este caso particular utilizo validaci√≥n cruzada con repeticiones, con k = 5 y 3 repeticiones. El argumento summaryFunction = twoClassSummary permite computar las m√©tricas necesarias (sensitividad y especificidad) para obtener ROC. Busque m√°s ayuda con help(\"trainControl\").\n\ntrain(): ajuste el modelo estableciendo la f√≥rmula habitual en R, el m√©todo o algoritmo para entrenar, (lista de algoritmos en caret) los datos, la configuraci√≥n para el entrenamiento (tcConrol = myControl) y la longitud de hiperpar√°metros a considerar en el entrenamiento (tuneLenth). Este √∫ltimo argumento depender√° de los hiperpar√°metros que est√©n disponibles en caret, aunque tambi√©n es posible asignarlos manualmente a trav√©s de expand.grid(). Utilizar el m√©todo igualado a ‚Äúrpart‚Äù permitir√° optimizar el par√°metro cp y utilizando ‚Äúrpart2‚Äù es posible optimizar la m√°xima profundidad del √°rbol. Cuando se declara tuneLength = 5 se informa que el n√∫mero m√°ximo de profundidades a probar ser√° 5, es decir, que al final existir√°n 5 resultados diferentes con el mismo algoritmo. Por √∫ltimo, se agrega la m√©trica que ser√° utilizada para comparar los resultados de la validaci√≥n cruzada.\n\n\n\nNota: como el procedimiento de validaci√≥n cruzada implica muestreo aleatorio, es necesario asignar la semilla para garantizar replicabilidad de resultados.\n\n\nC√≥digomyControl <- trainControl(method = \"repeatedcv\",\n                          number = 5,\n                          repeats = 3,\n                          classProbs = TRUE,  # Permite predecir probabilidades\n                          summaryFunction = twoClassSummary) \nset.seed(1992)\nmodArbol_tune <- train(class ~ .,\n                       method = \"rpart2\",\n                       data = data,\n                       trControl = myControl,\n                       tuneLength = 5,\n                       metric = \"ROC\")\nmodArbol_tune\n\nCART \n\n1000 samples\n  20 predictor\n   2 classes: 'bad', 'good' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold, repeated 3 times) \nSummary of sample sizes: 800, 800, 800, 800, 800, 800, ... \nResampling results across tuning parameters:\n\n  maxdepth  ROC        Sens       Spec     \n   3        0.7028968  0.3011111  0.8885714\n   6        0.7138095  0.3244444  0.8880952\n  11        0.7211151  0.3988889  0.8642857\n  14        0.7213571  0.4055556  0.8623810\n  18        0.7211746  0.3911111  0.8671429\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was maxdepth = 14.\n\n\n\nSe observa que la mejor profundidad es 14 con la mayor sensitividad a√∫n cuando no tiene la mejor especificidad. A continuaci√≥n la matriz de confusi√≥n en el conjunto de test muestra mejoras en la capacidad de detectar los clasificados como ‚Äúmalos‚Äù, ademas la precisi√≥n es notablemente superior.\n\n\nC√≥digopredichos_tune <- predict(object = modArbol_tune, newdata = dataTest, type = \"raw\")\nconfusionMatrix(data = predichos_tune, reference = as.factor(dataTest$class),\n                positive = \"good\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction bad good\n      bad   52   29\n      good  38  181\n                                          \n               Accuracy : 0.7767          \n                 95% CI : (0.7253, 0.8225)\n    No Information Rate : 0.7             \n    P-Value [Acc > NIR] : 0.001839        \n                                          \n                  Kappa : 0.4526          \n                                          \n Mcnemar's Test P-Value : 0.328393        \n                                          \n            Sensitivity : 0.8619          \n            Specificity : 0.5778          \n         Pos Pred Value : 0.8265          \n         Neg Pred Value : 0.6420          \n             Prevalence : 0.7000          \n         Detection Rate : 0.6033          \n   Detection Prevalence : 0.7300          \n      Balanced Accuracy : 0.7198          \n                                          \n       'Positive' Class : good            \n                                          \n\n\n\n\n√Årea bajo la curva el modelo evidentemente consigue mejores resultados respecto a los ajustados inicialmente, de tal manera que el ajuste de hiperpar√°metros ha logrado mejorar nuestras predicciones en datos que el modelo a√∫n no ha visto. Posiblemente el hecho de ajustar la m√°xima profundidad sumado al uso de validaci√≥n cruzada, permite que el modelo capture de mejor manera las relaciones subyacentes entre caracter√≠sticas.\n\n\nC√≥digo# Probabilidades predichas para la clase \"good\"\npred_tune <- as.data.frame(predict(object = modArbol_tune,\n                                   newdata = dataTest, type = \"prob\"))$good\n\n# AUC\nMetrics::auc(actual = target, predicted = pred_tune)\n\n[1] 0.7888624\n\n\n\nCurva ROC:\n\n\nC√≥digo# Ver funci√≥n myROC() al final en material complementario     \nmyROC(predichos = pred_tune, reales = target)"
  },
  {
    "objectID": "posts/tree_decision/TreeD_R.html#funci√≥n-plot-caret",
    "href": "posts/tree_decision/TreeD_R.html#funci√≥n-plot-caret",
    "title": "√Årbol de clasificaci√≥n con R",
    "section": "Funci√≥n plot + caret\n",
    "text": "Funci√≥n plot + caret\n\n\nLa funci√≥n plot() tiene un m√©todo espec√≠fico para resultados obtenidos a trav√©s de caret, en este caso muestra el gr√°fico del hiperpar√°metro de inter√©s, la m√°xima profundidad del √°rbol vs la curva ROC en el eje Y, tratando de evidenicar el valor √≥ptimo.\n\n\nC√≥digoplot(modArbol_tune)"
  },
  {
    "objectID": "posts/tree_decision/TreeD_R.html#gr√°fico-interactivo-con-visnetwork",
    "href": "posts/tree_decision/TreeD_R.html#gr√°fico-interactivo-con-visnetwork",
    "title": "√Årbol de clasificaci√≥n con R",
    "section": "Gr√°fico interactivo con visNetwork\n",
    "text": "Gr√°fico interactivo con visNetwork\n\n\nLa biblioteca visNetwork permite crear gr√°ficos interactivos para objetos de la clase rpart. A manera de ejemplo se presenta el gr√°fico para el √°rbol de decisi√≥n con poda. Recuerde que es interactivo y puede manipularlo con el mouse.\n\n\nC√≥digolibrary(visNetwork)\nvisTree(modArbol0_prune, \n        main = \"√Årbol con poda\"\", width==\"100%\"\",\n        height = \"800px\",  edgesFontSize = 14, nodesFontSize = 16,)"
  },
  {
    "objectID": "posts/tree_decision/TreeD_R.html#funci√≥n-myroc",
    "href": "posts/tree_decision/TreeD_R.html#funci√≥n-myroc",
    "title": "√Årbol de clasificaci√≥n con R",
    "section": "Funci√≥n myROC()\n",
    "text": "Funci√≥n myROC()\n\n\nEs necesario tener cargadas las bibliotecaS dplyr, ggplot2, hrbrthemes, Metrics y pROC para ejecutar la funci√≥n.\n\n\nC√≥digomyROC <- function(predichos, reales) {\n  suppressMessages(suppressWarnings(library(dplyr)))\n  suppressMessages(suppressWarnings(library(ggplot2)))\n  suppressMessages(suppressWarnings(library(pROC)))\n  suppressMessages(suppressWarnings(library(Metrics)))\n  x = roc(reales, predichos)\n  df = data_frame(TPR = x$sensitivities,\n                  FPR = 1 - x$specificities)\n  gg = df %>%\n    ggplot(aes(x = FPR, ymin = 0, ymax = TPR)) +\n    geom_polygon(aes(y = TPR), fill = \"#5A5156\", alpha = 0.7) +\n    geom_path(aes(y = TPR), col = \"#F6222E\", size = 1.3) +\n    geom_abline(\n      intercept = 0,\n      slope = 1,\n      color = \"gray37\",\n      size = 1,\n      linetype = \"dashed\"\n    ) +\n    theme_ipsum() +\n    coord_equal() +\n    labs(\n      x = \"FPR (1 - Especificidad)\",\n      y = \"TPR (Sensitividad)\",\n      title = paste0(\"Curva ROC\"),\n      subtitle = paste0(\n        \"Valor AUC: \",\n        Metrics::auc(actual = reales,\n                     predicted = predichos) %>% round(4)\n      )\n    )\n  return(gg)\n}"
  },
  {
    "objectID": "posts/XGBoost_R/XGBoost_R.html",
    "href": "posts/XGBoost_R/XGBoost_R.html",
    "title": "XGBoost con R",
    "section": "",
    "text": "Documentaci√≥n oficial xgboost\nBiblioteca xgboost en R\nTutorial de xgboost en R"
  },
  {
    "objectID": "posts/XGBoost_R/XGBoost_R.html#detecci√≥n-de-mastitis",
    "href": "posts/XGBoost_R/XGBoost_R.html#detecci√≥n-de-mastitis",
    "title": "XGBoost con R",
    "section": "Detecci√≥n de mastitis",
    "text": "Detecci√≥n de mastitis\n\nArt√≠culo publicado en PLOS ONE"
  },
  {
    "objectID": "posts/XGBoost_R/XGBoost_R.html#base-de-datos",
    "href": "posts/XGBoost_R/XGBoost_R.html#base-de-datos",
    "title": "XGBoost con R",
    "section": "Base de datos",
    "text": "Base de datos\n\nC√≥digomastitis <- read_csv(\"data/mastitis.csv\") %>% \n  clean_names() %>% \n  filter(diagnosis %in% c(\"EDP\", \"EL\"))\n\nmastitis %>% \n  select(diagnosis, 1:5) %>%\n  head() %>%\n  kable(caption = \"6 primeras filas de la base de datos con 6 columnas\")\n\n\n6 primeras filas de la base de datos con 6 columnas\n\n\n\n\n\n\n\n\n\ndiagnosis\nquarter_dates_q0\nno_recordings_q0\nl_1_q0_bmscc_000_cells_ml\nl_1_q0_percent_chronic\nl_1_q0_percent_200k\n\n\n\nEDP\n06/10/2009\n0\n0\n0.0\n0.0\n\n\nEDP\n06/12/2009\n3\n284\n27.8\n40.6\n\n\nEL\n20/04/2012\n0\n0\n0.0\n0.0\n\n\nEL\n16/07/2009\n3\n222\n21.1\n26.3\n\n\nEL\n16/10/2013\n3\n251\n27.5\n34.3\n\n\nEDP\n12/01/2013\n3\n154\n12.3\n17.0"
  },
  {
    "objectID": "posts/XGBoost_R/XGBoost_R.html#an√°lisis-exploratorio",
    "href": "posts/XGBoost_R/XGBoost_R.html#an√°lisis-exploratorio",
    "title": "XGBoost con R",
    "section": "An√°lisis exploratorio",
    "text": "An√°lisis exploratorio\n\nFrecuencia absoluta para niveles de la variable respuesta.\n\n\nC√≥digomastitis %>% \n  count(diagnosis) %>% \n  ggplot(aes(x = diagnosis, y = n)) +\n  geom_col(color = \"dodgerblue3\", fill = \"dodgerblue3\", alpha = 0.8) +\n  geom_label(aes(label = n)) +\n  labs(x = \"Diagn√≥stico\"\", y==\"Frecuencia\"))\n\n\n\nFrecuencia absoluta de variable respuesta\n\n\n\n\n\nDistribuciones de algunas variables num√©ricas: como el n√∫mero de variables es alto, selecciono al azar 8 de ellas para construir el gr√°fico.\n\n\nC√≥digo# N√∫meros al azarr\nset.seed(1992)\nvariables_azar <- sample(x = 228, size = 8, replace = FALSE)\n\nmastitis %>% \n  select(diagnosis, variables_azar) %>% \n  pivot_longer(cols = -diagnosis) %>% \n  ggplot(aes(x = value, fill = diagnosis, color = diagnosis)) +\n  facet_wrap(~name, scales = \"free\", ncol = 4) +\n  geom_density(alpha = 0.8) +\n  scale_x_log10() +\n  scale_color_jama() +\n  scale_fill_jama() +\n  labs(x = \"\", y = \"Densidad\", color = \"Diagn√≥stico\"\",\n       fill = \"Diagn√≥stico\"))\n\n\n\nDensidades de 8 variables al azar (transformaci√≥n con logaritmo en base 10)\n\n\n\n\n\nVerificamos si existen valores ausentes. Se observan algunas filas con valores NA.\n\n\nC√≥digovis_miss(mastitis)\n\n\n\nValores ausentes en el conjunto de datos\n\n\n\n\n\nCon la finalidad de evidenciar si existe alg√∫n patr√≥n de asociaci√≥n subyacente en el total de variables, se realiz√≥ an√°lisis de componentes principales y se grafican los dos primeros componentes. No existe alg√∫n comportamiento de agrupaci√≥n al reducir la dimensi√≥n a los dos primeros componentes. Se observ√≥ que la retenci√≥n de variabilidad de estas dos coordenadas apenas alcanz√≥ el 30% aproximadamente.\n\n\nC√≥digo# An√°lisis de componentes principaless\ndatos_pca <- mastitis\npca <- PCA(X = datos_pca %>%\n             select(where(is.numeric)),\n           scale.unit = TRUE,\n           graph = FALSE)\n\n# Agregando componentes a la base de datos\ndatos_pca$cp1 <- pca$ind$coord[, 1]\ndatos_pca$cp2 <- pca$ind$coord[, 2]\n\n# Gr√°fico de las 2 primeras componentess\ndatos_pca %>% \n  ggplot(aes(x = cp1, y = cp2, color = diagnosis)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"black\", linetype = 2) +\n  geom_vline(xintercept = 0, color = \"black\", linetype = 2) +\n  scale_color_jama() +\n  labs(x = \"CP1 (15.74 %)\", y = \"CP2 (14.09 %)\", color = \"Diagn√≥stico\"))\n\n\n\nComponente principal 1 vs componente principal 2\n\n\n\n\n\nLos resultados exploratorios sugieren cuatro cosas importantes:\n\nEl problema de clasificaci√≥n bajo an√°lisis podr√≠a ser denominado de clases balanceadas, ya que las frecuencias absolutas son similares para cada nivel a predecir.\nAlgunas variables predictoras no tienen comportamiento gaussiano. Parte de la estrategia del an√°lisis, en estos casos, podr√≠a ser la implementaci√≥n de alg√∫n tipo de transformaci√≥n previo al entrenamiento de los modelos, no obstante, este paso no se aplicar√° dada la robustez que presenta el algoritmo XGBoost frente a distribuciones asim√©tricas.\nExisten observaciones con valores ausentes para una o m√°s variables. Algunos algoritmos como la regresi√≥n log√≠stica o los bosques aleatorios, no permiten la inclusi√≥n de valores NA al entrenar los modelos, sin embargo, con XGBoost no existe este inconveniente, puesto que soporta valores vac√≠os.\nEl an√°lisis de componentes principales no muestra alguna tendencia de agrupaci√≥n entre las clases evaluadas. La retenci√≥n de variabilidad de las tres primeras componentes no supera el 50%, resultado que podr√≠a sugerir que las relaciones de tipo lineal no son plausibles en este conjunto de datos."
  },
  {
    "objectID": "posts/XGBoost_R/XGBoost_R.html#train-y-test",
    "href": "posts/XGBoost_R/XGBoost_R.html#train-y-test",
    "title": "XGBoost con R",
    "section": "Train y Test",
    "text": "Train y Test\n\nEn este ejemplo los datos fueron divididos en train y test con proporciones de 80 y 20 %, respectivamente. Se utiliza muestreo estratificado en funci√≥n de la variable respuesta. Se eliminaron 4 variables que tienen informaci√≥n de fechas; aunque podr√≠an ser tratadas de alguna manera especial, en este caso no fueron tenidas en cuenta.\nLa biblioteca rsample permite realizar la divisi√≥n.\n\n\nC√≥digo# Variables fechas\nvariables_fechas <- mastitis %>% \n  select(is.character, -diagnosis) %>% \n  names()\n\n# Datos para modelos\ndata_modelos <- mastitis %>% select(-variables_fechas)\n\nset.seed(1992)\nparticiones <- initial_split(data = data_modelos, prop = 0.80, strata = diagnosis)\ntrain <- training(particiones)\ntest <- testing(particiones)\n\n\n\nPodemos ver el objeto particiones que proporciona informaci√≥n de la partici√≥n de datos. Los modelos son entrenados con 733 observaciones y con 181 se eval√∫a el desempe√±o de los mismos.\n\n\nC√≥digoparticiones\n\n<Training/Testing/Total>\n<731/183/914>"
  },
  {
    "objectID": "posts/XGBoost_R/XGBoost_R.html#validaci√≥n-cruzada",
    "href": "posts/XGBoost_R/XGBoost_R.html#validaci√≥n-cruzada",
    "title": "XGBoost con R",
    "section": "Validaci√≥n cruzada",
    "text": "Validaci√≥n cruzada\n\nSe utiliza validaci√≥n cruzada k-fold con \\(k = 10\\).\nLa biblioteca rsample permite configurar diferentes m√©todos de validaci√≥n cruzada.\n\n\nC√≥digoset.seed(1992)\nconfig_cv <- vfold_cv(data = train, v = 10)"
  },
  {
    "objectID": "posts/XGBoost_R/XGBoost_R.html#preprocesamiento",
    "href": "posts/XGBoost_R/XGBoost_R.html#preprocesamiento",
    "title": "XGBoost con R",
    "section": "Preprocesamiento",
    "text": "Preprocesamiento\n\nEn este ejemplo no se har√° √©nfasis en las estrategias de preprocesamiento o ingenier√≠a de caracter√≠sticas, sin embargo, ser√°n comparados algoritmos con imputaci√≥n de valores ausentes (con m√©todo k vecinos m√°s cercanos) respecto a algoritmos sin imputaci√≥n.\nLa biblioteca recipes permite realizar m√∫ltiples tareas de preprocesamiento e ingenier√≠a de caracter√≠sticas.\n\n\nC√≥digono_impute <- recipe(diagnosis ~ ., data = train)\nsi_impute <- recipe(diagnosis ~ ., data = train) %>% \n  step_impute_knn(all_predictors())"
  },
  {
    "objectID": "posts/XGBoost_R/XGBoost_R.html#modelo-xgboost",
    "href": "posts/XGBoost_R/XGBoost_R.html#modelo-xgboost",
    "title": "XGBoost con R",
    "section": "Modelo XGBoost",
    "text": "Modelo XGBoost\n\nEl algoritmo XGboost tiene m√∫ltiples hiperpar√°metros que pueden ser sintonizados, sin embargo, en este ejemplo s√≥lo ser har√° tuning sobre los siguientes:\n\n\nmtry: n√∫mero de predictores que se muestrear√°n aleatoriamente en cada divisi√≥n al crear los modelos.\n\nmin_n: n√∫mero m√≠nimo de observaciones requeridas en un nodo para que se produzca la divisi√≥n.\n\ntree_depth: profundidad m√°xima del √°rbol (n√∫mero de divisiones).\n\n\nEl n√∫mero de √°rboles (trees) se estableci√≥ en 1000.\nLa tasa de aprendizaje (learn_rate) se estableci√≥ en 0.1.\nLa proporci√≥n de observaciones muestreadas (sample_size) en cada rutina de ajuste se estableci√≥ en 0.8.\nLos dem√°s hiperpar√°metros se mantienen por defecto.\nPara m√°s informaci√≥n acerca de los hiperpar√°metros que permite ajustar parsnip, consultar este enlace.\n\n\n\nC√≥digomodelo_xgboost <- boost_tree(mtry = tune(),\n                             min_n = tune(),\n                             tree_depth = tune(),\n                             trees = 1000,\n                             learn_rate = 0.1,\n                             sample_size = 0.8) %>% \n  set_engine(\"xgboost\") %>% \n  set_mode(\"classification\")"
  },
  {
    "objectID": "posts/XGBoost_R/XGBoost_R.html#flujos-de-trabajo-pipelines",
    "href": "posts/XGBoost_R/XGBoost_R.html#flujos-de-trabajo-pipelines",
    "title": "XGBoost con R",
    "section": "Flujos de trabajo (pipelines)",
    "text": "Flujos de trabajo (pipelines)\n\nLos flujos de trabajo son una manera flexible de trabajar con tidymodels. Se fundamenta en la misma idea de los pipelines de scikit-learn de Python. Es posible construir nuestro flujo de trabajo con recetas y modelos declarados previamente.\nConsultar m√°s informaci√≥n de la biblioteca workflows.\nFlujo de trabajo sin imputaci√≥n:\n\n\nC√≥digowf_no_impute <- workflow() %>% \n  add_recipe(no_impute) %>% \n  add_model(modelo_xgboost)\n\n\n\nFlujo de trabajo con imputaci√≥n:\n\n\nC√≥digowf_si_impute <- workflow() %>% \n  add_recipe(si_impute) %>% \n  add_model(modelo_xgboost)"
  },
  {
    "objectID": "posts/XGBoost_R/XGBoost_R.html#grid",
    "href": "posts/XGBoost_R/XGBoost_R.html#grid",
    "title": "XGBoost con R",
    "section": "Grid",
    "text": "Grid\n\nEn este caso se utiliza la cuadr√≠cula a trav√©s de dise√±os de llenado de espacio (space-filling designs), los cuales intentan encontrar una configuraci√≥n de puntos (combinaciones) que cubren el espacio de par√°metros con menor probabilidad de valores que se traslapan. Para este ejemplo el tama√±o de la cuadr√≠cula fue de 10.\nPara este caso particular se usan dise√±os de m√°xima entrop√≠a, descritos en el a√±o 1987 por Shewry y Wynn en el art√≠culo ‚ÄúMaximum Entropy Sampling‚Äù. Tambi√©n podr√≠an ser implementados dise√±os de hipercubos latinos o dise√±os de proyecci√≥n m√°xima. Si no se desea utilizar alguno de estos m√©todos, podr√≠a ser implementada una cuadr√≠cula regular a trav√©s de m√©todos aleatorios (grid random).\nMayor informaci√≥n en la p√°gina web de la biblioteca dials.\n\n\n\nC√≥digo# Par√°metros para tuningg\nparams_xgb <- parameters(\n  finalize(mtry(), x = train[, -1]),\n  min_n(range = c(2L, 50L)),\n  tree_depth(range = c(3L, 8L))\n)\n\n# Grid\nset.seed(2021)\ngrid_xgb <- params_xgb %>% \n  grid_max_entropy(size = 10)\n\n\n\nA continuaci√≥n se muestra la cuadr√≠cula de b√∫squeda de los mejores hiperpar√°metros. Se evidencia que los puntos no se solapan, de tal manera que el espacio de b√∫squeda no es redundante.\n\n\nC√≥digogrid_xgb %>% \n  ggplot(aes(x = .panel_x, y = .panel_y)) +\n  facet_matrix(vars(mtry, min_n, tree_depth), layer.diag = 2) +\n  geom_point()\n\n\n\nCuadr√≠cula de tama√±o 10 con m√©todo de m√°xima entrop√≠a"
  },
  {
    "objectID": "posts/XGBoost_R/XGBoost_R.html#tuning-con-tidymodels",
    "href": "posts/XGBoost_R/XGBoost_R.html#tuning-con-tidymodels",
    "title": "XGBoost con R",
    "section": "Tuning con tidymodels\n",
    "text": "Tuning con tidymodels\n\n\nLa funci√≥n tune_grid() de la biblioteca tune permite evaluar los modelos con cada combinaci√≥n de param√©tros establecidos previamente en la cuadr√≠cula.\nTuning sin imputaci√≥n:\n\n\nC√≥digoregisterDoParallel(parallel::detectCores() - 1) # Inicio Paralelizaci√≥nn\n\nset.seed(2021)\ntuned_no_impute <- tune_grid(\n  object = wf_no_impute,\n  resamples = config_cv,\n  grid = grid_xgb\n)\n\nstopImplicitCluster() # Fin Paralelizaci√≥nn\n\n\n\nTuning con imputaci√≥n:\n\n\nC√≥digoregisterDoParallel(parallel::detectCores() - 1) # Inicio Paralelizaci√≥nn\n\nset.seed(2021)\ntuned_si_impute <- tune_grid(\n  object = wf_si_impute,\n  resamples = config_cv,\n  grid = grid_xgb\n)\n\nstopImplicitCluster() # Fin Paralelizaci√≥nn"
  },
  {
    "objectID": "posts/XGBoost_R/XGBoost_R.html#resultados-accuracy",
    "href": "posts/XGBoost_R/XGBoost_R.html#resultados-accuracy",
    "title": "XGBoost con R",
    "section": "Resultados Accuracy\n",
    "text": "Resultados Accuracy\n\n\nResultados de Accuracy en modelos sin imputaci√≥n. La precisi√≥n m√°s alta se consigue con aproximadamente 150 variables (mtry), menos de 10 observaciones para que se produzca la divisi√≥n del √°rbol (min_n) y profunidad de m√°s o menos 6 (tree_depth).\n\n\nC√≥digotuned_no_impute %>% \n  collect_metrics() %>% \n  filter(.metric == \"accuracy\") %>% \n  ggplot(aes(x = mtry, y = min_n, size = tree_depth, color = mean)) +\n  geom_point() +\n  scale_color_viridis_c() +\n  labs(color = \"Accuracy\")\n\n\n\n\n\nResultados de Accuracy en modelos con imputaci√≥n. La precisi√≥n m√°s alta se consigue con aproximadamente 25 variables (mtry), poco menos de 50 observaciones para que se produzca la divisi√≥n del √°rbol (min_n) y profunidad de m√°s o menos 4 (tree_depth).\n\n\nC√≥digotuned_si_impute %>% \n  collect_metrics() %>% \n  filter(.metric == \"accuracy\") %>% \n  ggplot(aes(x = mtry, y = min_n, size = tree_depth, color = mean)) +\n  geom_point() +\n  scale_color_viridis_c() +\n  labs(color = \"Accuracy\")\n\n\n\n\n\nSe observa que los mejores hiperpar√°metros para los algoritmos entrenados con y sin imputaci√≥n, discrepan considerablemente. Aunque en este caso realic√© la evaluaci√≥n del desempe√±o de los modelos basado en la m√©trica Accuracy, es posible utilizar cualquier otra para problemas de clasificaci√≥n."
  },
  {
    "objectID": "posts/XGBoost_R/XGBoost_R.html#mejores-hiperpar√°metros",
    "href": "posts/XGBoost_R/XGBoost_R.html#mejores-hiperpar√°metros",
    "title": "XGBoost con R",
    "section": "Mejores hiperpar√°metros",
    "text": "Mejores hiperpar√°metros\n\nMejores hiperpar√°metros en modelos sin imputaci√≥n:\n\n\nC√≥digomejor_no_impute <- tuned_no_impute %>% \n  select_best(metric = \"accuracy\")\n\nmejor_no_impute\n\n\n\n  \n\n\n\n\nMejores hiperpar√°metros en modelos con imputaci√≥n:\n\n\nC√≥digomejor_si_impute <- tuned_si_impute %>% \n  select_best(metric = \"accuracy\")\n\nmejor_si_impute"
  },
  {
    "objectID": "posts/XGBoost_R/XGBoost_R.html#ajuste-final",
    "href": "posts/XGBoost_R/XGBoost_R.html#ajuste-final",
    "title": "XGBoost con R",
    "section": "Ajuste final",
    "text": "Ajuste final\n\nModelo sin imputaci√≥n:\n\n\nC√≥digofinal_no_impute <- finalize_workflow(\n  x = wf_no_impute,\n  parameters = mejor_no_impute\n) %>% \n  fit(data = train)\n\n\n\nModelo con imputaci√≥n:\n\n\nC√≥digofinal_si_impute <- finalize_workflow(\n  x = wf_si_impute,\n  parameters = mejor_si_impute\n) %>% \n  fit(data = train)"
  },
  {
    "objectID": "posts/XGBoost_R/XGBoost_R.html#predicciones-train",
    "href": "posts/XGBoost_R/XGBoost_R.html#predicciones-train",
    "title": "XGBoost con R",
    "section": "Predicciones Train",
    "text": "Predicciones Train\n\nModelo sin imputaci√≥n:\n\n\nC√≥digopred_no_impute_train <- final_no_impute %>% \n  predict(new_data = train, type = \"class\")\n\n\n\nModelo con imputaci√≥n:\n\n\nC√≥digopred_si_impute_train <- final_si_impute %>% \n  predict(new_data = train, type = \"class\")"
  },
  {
    "objectID": "posts/XGBoost_R/XGBoost_R.html#predicciones-test",
    "href": "posts/XGBoost_R/XGBoost_R.html#predicciones-test",
    "title": "XGBoost con R",
    "section": "Predicciones Test",
    "text": "Predicciones Test\n\nModelo sin imputaci√≥n:\n\n\nC√≥digopred_no_impute_test <- final_no_impute %>% \n  predict(new_data = test, type = \"class\")\n\n\n\nModelo con imputaci√≥n:\n\n\nC√≥digopred_si_impute_test <- final_si_impute %>% \n  predict(new_data = test, type = \"class\")"
  },
  {
    "objectID": "posts/XGBoost_R/XGBoost_R.html#matriz-de-confusi√≥n-train",
    "href": "posts/XGBoost_R/XGBoost_R.html#matriz-de-confusi√≥n-train",
    "title": "XGBoost con R",
    "section": "Matriz de confusi√≥n Train",
    "text": "Matriz de confusi√≥n Train\n\nMatriz de confusi√≥n modelo sin imputaci√≥n:\n\n\nC√≥digodata.frame(\n  predicho = as.factor(pred_no_impute_train$.pred_class),\n  real = as.factor(train$diagnosis)\n) %>% \n  conf_mat(truth = real, estimate = predicho) %>% \n  pluck(1) %>% \n  as_tibble()  %>% \n  ggplot(aes(x = Prediction, y = Truth, alpha = n)) +\n  geom_tile(show.legend = FALSE) +\n  geom_text(aes(label = n), colour = \"white\", alpha = 1, size = 8)\n\n\n\nMatriz de confusi√≥n en train - modelo sin imputaci√≥n\n\n\n\n\n\nMatriz de confusi√≥n modelo con imputaci√≥n:\n\n\nC√≥digodata.frame(\n  predicho = as.factor(pred_si_impute_train$.pred_class),\n  real = as.factor(train$diagnosis)\n) %>% \n  conf_mat(truth = real, estimate = predicho) %>% \n  pluck(1) %>% \n  as_tibble()  %>% \n  ggplot(aes(x = Prediction, y = Truth, alpha = n)) +\n  geom_tile(show.legend = FALSE) +\n  geom_text(aes(label = n), colour = \"white\", alpha = 1, size = 8)\n\n\n\nMatriz de confusi√≥n en train - modelo con imputaci√≥n"
  },
  {
    "objectID": "posts/XGBoost_R/XGBoost_R.html#matriz-de-confusi√≥n-test",
    "href": "posts/XGBoost_R/XGBoost_R.html#matriz-de-confusi√≥n-test",
    "title": "XGBoost con R",
    "section": "Matriz de confusi√≥n Test",
    "text": "Matriz de confusi√≥n Test\n\nMatriz de confusi√≥n modelo sin imputaci√≥n:\n\n\nC√≥digodata.frame(\n  predicho = as.factor(pred_no_impute_test$.pred_class),\n  real = as.factor(test$diagnosis)\n) %>% \n  conf_mat(truth = real, estimate = predicho) %>% \n  pluck(1) %>% \n  as_tibble()  %>% \n  ggplot(aes(x = Prediction, y = Truth, alpha = n)) +\n  geom_tile(show.legend = FALSE) +\n  geom_text(aes(label = n), colour = \"white\", alpha = 1, size = 8)\n\n\n\nMatriz de confusi√≥n en test - modelo sin imputaci√≥n\n\n\n\n\n\nMatriz de confusi√≥n modelo con imputaci√≥n:\n\n\nC√≥digodata.frame(\n  predicho = as.factor(pred_si_impute_test$.pred_class),\n  real = as.factor(test$diagnosis)\n) %>% \n  conf_mat(truth = real, estimate = predicho) %>% \n  pluck(1) %>% \n  as_tibble()  %>% \n  ggplot(aes(x = Prediction, y = Truth, alpha = n)) +\n  geom_tile(show.legend = FALSE) +\n  geom_text(aes(label = n), colour = \"white\", alpha = 1, size = 8)\n\n\n\nMatriz de confusi√≥n en test - modelo con imputaci√≥n"
  },
  {
    "objectID": "posts/XGBoost_R/XGBoost_R.html#desempe√±o-de-modelos",
    "href": "posts/XGBoost_R/XGBoost_R.html#desempe√±o-de-modelos",
    "title": "XGBoost con R",
    "section": "Desempe√±o de modelos",
    "text": "Desempe√±o de modelos\n\nGenero una base de datos con los resultados de las clases predichas en cada tipo de modelo (con y sin imputaci√≥n) para los conjuntos de entrenamiento y prueba.\n\n\nC√≥digotabla_accuracy <- data.frame(\n  predicho = pred_no_impute_train$.pred_class,\n  real = train$diagnosis,\n  datos = \"Train\",\n  tipo = \"Sin imputaci√≥n\"\"\n) %>%\n  bind_rows(\n    data.frame(\n      predicho = pred_si_impute_train$.pred_class,\n      real = train$diagnosis,\n      datos = \"Train\",\n      tipo = \"Con imputaci√≥n\"\"\n    )\n  ) %>%\n  bind_rows(\n    data.frame(\n      predicho = pred_no_impute_test$.pred_class,\n      real = test$diagnosis,\n      datos = \"Test\",\n      tipo = \"Sin imputaci√≥n\"\"\n    )\n  ) %>% \n  bind_rows(\n    data.frame(\n      predicho = pred_si_impute_test$.pred_class,\n      real = test$diagnosis,\n      datos = \"Test\",\n      tipo = \"Con imputaci√≥n\"\"\n    )\n  ) %>% \n  mutate(across(where(is.character), as.factor))\n\ntabla_accuracy %>%\n  group_by(datos, tipo) %>% \n  summarise(accuracy = accuracy_vec(truth = real, estimate = predicho)) %>% \n  kable(caption = \"Accuracy en train y test para dos modelos XGBoost\")\n\n\nAccuracy en train y test para dos modelos XGBoost\n\ndatos\ntipo\naccuracy\n\n\n\nTest\nCon imputaci√≥n\n0.7540984\n\n\nTest\nSin imputaci√≥n\n0.7814208\n\n\nTrain\nCon imputaci√≥n\n0.8481532\n\n\nTrain\nSin imputaci√≥n\n0.9972640\n\n\n\n\n\n\nGr√°fico Accuracy: la capacidad predictiva es superior en el modelo que fue entrenado sin acudir a la imputaci√≥n de datos.\n\n\nC√≥digotabla_accuracy %>%\n  group_by(datos, tipo) %>% \n  summarise(accuracy = accuracy_vec(truth = real, estimate = predicho)) %>% \n  ggplot(aes(x = tipo, y = accuracy, color = datos, fill = datos)) +\n  geom_col(position = \"dodge\", alpha = 0.8) +\n  scale_color_jama() +\n  scale_fill_jama() +\n  labs(x = \"Preprocesamiento\", y = \"Accuracy\",\n       color = \"\", fill = \"\")\n\n\n\nAccuracy en train y test para dos modelos XGBoost"
  },
  {
    "objectID": "posts/XGBoost_R/XGBoost_R.html#importancia-de-variables",
    "href": "posts/XGBoost_R/XGBoost_R.html#importancia-de-variables",
    "title": "XGBoost con R",
    "section": "Importancia de variables",
    "text": "Importancia de variables\n\n10 variables de mayor importancia en modelo sin imputaci√≥n:\n\n\nC√≥digofinal_no_impute %>% \n  pull_workflow_fit() %>%\n  vip(geom = \"point\", n = 10)\n\n\n\nImportancia de variables - modelo sin imputaci√≥n\n\n\n\n\n\n10 variables de mayor importancia en modelo sin imputaci√≥n:\n\n\nC√≥digofinal_si_impute %>% \n  pull_workflow_fit() %>%\n  vip(geom = \"point\", n = 10)\n\n\n\nImportancia de variables - modelo con imputaci√≥n"
  }
]